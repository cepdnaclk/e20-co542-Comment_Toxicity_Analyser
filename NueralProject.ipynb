{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LcSkRJz5ykpn"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc-atc5ift0t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import os # navigate through defferent files\n",
        "import numpy as np\n",
        "import wandb\n",
        "import mlflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.backend import epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh7f2sKOtsv-"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "snVGaXC3yVuU",
        "outputId": "57f92d07-e832-42c1-a93f-74e82110b86d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me20189\u001b[0m (\u001b[33me20189-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250306_065352-5npahudd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/5npahudd' target=\"_blank\">curious-bee-4</a></strong> to <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/5npahudd' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/5npahudd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/5npahudd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78e896ae6050>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "import mlflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize Weights & Biases (W&B)\n",
        "wandb.login()\n",
        "wandb.init(project=\"toxicity-detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UsNoiYSjy4d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/CommentToxicity/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv') # train data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "knZGZdeqkvOl",
        "outputId": "cf499a39-8bbb-4707-e550-765fb97765da"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df[df['toxic'] ==1]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0005c987bdfc9d4b\",\n          \"00190820581d90ce\",\n          \"0007e25b2121310b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\r\\n\\r\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...\",\n          \"FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\",\n          \"Bye! \\r\\n\\r\\nDon't look, come or think of comming back! Tosser.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"severe_toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"obscene\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insult\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity_hate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2eb40eae-fdd7-4c17-b158-c330786c8e2d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0005c987bdfc9d4b</td>\n",
              "      <td>Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0007e25b2121310b</td>\n",
              "      <td>Bye! \\r\\n\\r\\nDon't look, come or think of comm...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>001810bf8c45bf5f</td>\n",
              "      <td>You are gay or antisemmitian? \\r\\n\\r\\nArchange...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>00190820581d90ce</td>\n",
              "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eb40eae-fdd7-4c17-b158-c330786c8e2d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2eb40eae-fdd7-4c17-b158-c330786c8e2d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2eb40eae-fdd7-4c17-b158-c330786c8e2d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-34cdd913-a966-4b91-bd18-6b2228d4cbf1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-34cdd913-a966-4b91-bd18-6b2228d4cbf1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-34cdd913-a966-4b91-bd18-6b2228d4cbf1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  id                                       comment_text  \\\n",
              "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
              "12  0005c987bdfc9d4b  Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...   \n",
              "16  0007e25b2121310b  Bye! \\r\\n\\r\\nDon't look, come or think of comm...   \n",
              "42  001810bf8c45bf5f  You are gay or antisemmitian? \\r\\n\\r\\nArchange...   \n",
              "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
              "\n",
              "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
              "6       1             1        1       0       1              0  \n",
              "12      1             0        0       0       0              0  \n",
              "16      1             0        0       0       0              0  \n",
              "42      1             0        1       0       1              1  \n",
              "43      1             0        1       0       1              0  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df['toxic'] ==1].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "TbZOxomWpt4e",
        "outputId": "e7da5a7e-7681-4478-da76-92d231994aae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\r\\n\\r\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id = 12\n",
        "df.iloc[id]['comment_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "E-RnzBmfpvat",
        "outputId": "f11302e3-6522-4224-81aa-49260316726d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>toxic</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>severe_toxic</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>obscene</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>threat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insult</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity_hate</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "toxic            1\n",
              "severe_toxic     0\n",
              "obscene          0\n",
              "threat           0\n",
              "insult           0\n",
              "identity_hate    0\n",
              "Name: 12, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df.columns[2:]].iloc[id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG8k2IdDtvfF"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq_2ORSstxqG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeN5A-x8uIEe",
        "outputId": "6cc524d4-943f-4bdf-80c7-66600b4dc00d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
              "       'insult', 'identity_hate'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJgkGkrbuvMd"
      },
      "outputs": [],
      "source": [
        "y = df[df.columns[2:]].values # converting the measuring values in the data frames to a matrix as labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKwoL0VKvaPe"
      },
      "outputs": [],
      "source": [
        "x = df['comment_text'] # take the comment text as inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeFwRH0-wahN"
      },
      "outputs": [],
      "source": [
        "MAX_WORDS = 200000 # Number of vocab in the dictionary\n",
        "vectorizer = TextVectorization(max_tokens=MAX_WORDS , output_sequence_length=1800 ,output_mode='int') # cap the max length of inputs to 1800 words and the model outputs data in the integer format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR_G6Np5xus2"
      },
      "outputs": [],
      "source": [
        "vectorizer.adapt(x.values) # fit the vectorizer to the data , learn the words in the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmVFRQBwfTDJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Save vectorizer weights\n",
        "vectorizer_weights = vectorizer.get_weights()\n",
        "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer_weights, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9h736xFhRzl",
        "outputId": "ef7cb5cd-0742-481a-d663-7995203cd8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Vocabulary saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "#  Save the vocabulary\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "with open(\"vectorizer_vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "print(\" Vocabulary saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSn7rwEty56O",
        "outputId": "92f2d0ad-d31d-4b31-cff8-0d876108af39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'to',\n",
              " 'of',\n",
              " 'and',\n",
              " 'a',\n",
              " 'you',\n",
              " 'i',\n",
              " 'is',\n",
              " 'that',\n",
              " 'in',\n",
              " 'it',\n",
              " 'for',\n",
              " 'this',\n",
              " 'not',\n",
              " 'on',\n",
              " 'be',\n",
              " 'as',\n",
              " 'have',\n",
              " 'are',\n",
              " 'your',\n",
              " 'with',\n",
              " 'if',\n",
              " 'article',\n",
              " 'was',\n",
              " 'or',\n",
              " 'but',\n",
              " 'page',\n",
              " 'my',\n",
              " 'an',\n",
              " 'from',\n",
              " 'by',\n",
              " 'do',\n",
              " 'at',\n",
              " 'about',\n",
              " 'me',\n",
              " 'so',\n",
              " 'wikipedia',\n",
              " 'can',\n",
              " 'what',\n",
              " 'there',\n",
              " 'all',\n",
              " 'has',\n",
              " 'will',\n",
              " 'talk',\n",
              " 'please',\n",
              " 'would',\n",
              " 'its',\n",
              " 'no',\n",
              " 'one',\n",
              " 'just',\n",
              " 'like',\n",
              " 'they',\n",
              " 'he',\n",
              " 'dont',\n",
              " 'which',\n",
              " 'any',\n",
              " 'been',\n",
              " 'should',\n",
              " 'more',\n",
              " 'we',\n",
              " 'some',\n",
              " 'other',\n",
              " 'who',\n",
              " 'see',\n",
              " 'here',\n",
              " 'also',\n",
              " 'his',\n",
              " 'think',\n",
              " 'im',\n",
              " 'because',\n",
              " 'know',\n",
              " 'how',\n",
              " 'am',\n",
              " 'people',\n",
              " 'why',\n",
              " 'edit',\n",
              " 'articles',\n",
              " 'only',\n",
              " 'out',\n",
              " 'up',\n",
              " 'when',\n",
              " 'were',\n",
              " 'use',\n",
              " 'then',\n",
              " 'may',\n",
              " 'time',\n",
              " 'did',\n",
              " 'them',\n",
              " 'now',\n",
              " 'being',\n",
              " 'their',\n",
              " 'than',\n",
              " 'thanks',\n",
              " 'even',\n",
              " 'get',\n",
              " 'make',\n",
              " 'good',\n",
              " 'had',\n",
              " 'very',\n",
              " 'information',\n",
              " 'does',\n",
              " 'could',\n",
              " 'well',\n",
              " 'want',\n",
              " 'such',\n",
              " 'sources',\n",
              " 'way',\n",
              " 'name',\n",
              " 'these',\n",
              " 'deletion',\n",
              " 'pages',\n",
              " 'first',\n",
              " 'help',\n",
              " 'new',\n",
              " 'editing',\n",
              " 'source',\n",
              " 'go',\n",
              " 'need',\n",
              " 'say',\n",
              " 'section',\n",
              " 'edits',\n",
              " 'again',\n",
              " 'thank',\n",
              " 'where',\n",
              " 'user',\n",
              " 'made',\n",
              " 'many',\n",
              " 'much',\n",
              " 'really',\n",
              " 'used',\n",
              " 'most',\n",
              " 'discussion',\n",
              " 'find',\n",
              " 'same',\n",
              " 'ive',\n",
              " 'deleted',\n",
              " 'into',\n",
              " 'fuck',\n",
              " 'those',\n",
              " 'work',\n",
              " 'since',\n",
              " 'before',\n",
              " 'after',\n",
              " 'point',\n",
              " 'add',\n",
              " 'look',\n",
              " 'right',\n",
              " 'read',\n",
              " 'image',\n",
              " 'take',\n",
              " 'still',\n",
              " 'over',\n",
              " 'someone',\n",
              " 'him',\n",
              " 'two',\n",
              " 'back',\n",
              " 'too',\n",
              " 'fact',\n",
              " 'link',\n",
              " 'said',\n",
              " 'own',\n",
              " 'something',\n",
              " 'going',\n",
              " 'youre',\n",
              " 'blocked',\n",
              " 'list',\n",
              " 'stop',\n",
              " 'without',\n",
              " 'content',\n",
              " 'hi',\n",
              " 'under',\n",
              " 'editors',\n",
              " 'our',\n",
              " 'block',\n",
              " 'thats',\n",
              " 'us',\n",
              " 'added',\n",
              " 'utc',\n",
              " 'history',\n",
              " 'another',\n",
              " 'doesnt',\n",
              " 'removed',\n",
              " 'might',\n",
              " 'note',\n",
              " 'however',\n",
              " 'sure',\n",
              " 'place',\n",
              " 'never',\n",
              " 'done',\n",
              " 'welcome',\n",
              " 'her',\n",
              " 'case',\n",
              " 'put',\n",
              " 'personal',\n",
              " 'seems',\n",
              " 'reason',\n",
              " 'better',\n",
              " 'using',\n",
              " 'yourself',\n",
              " 'cant',\n",
              " 'actually',\n",
              " 'ask',\n",
              " 'comment',\n",
              " 'while',\n",
              " 'vandalism',\n",
              " 'feel',\n",
              " 'question',\n",
              " 'anything',\n",
              " 'believe',\n",
              " 'person',\n",
              " 'links',\n",
              " 'things',\n",
              " 'both',\n",
              " 'didnt',\n",
              " 'comments',\n",
              " 'best',\n",
              " 'ill',\n",
              " 'part',\n",
              " 'she',\n",
              " 'hope',\n",
              " 'policy',\n",
              " 'against',\n",
              " 'off',\n",
              " 'keep',\n",
              " 'already',\n",
              " 'free',\n",
              " 'wiki',\n",
              " 'thing',\n",
              " 'nothing',\n",
              " 'change',\n",
              " 'wrong',\n",
              " 'though',\n",
              " 'problem',\n",
              " 'remove',\n",
              " 'little',\n",
              " 'subject',\n",
              " '•',\n",
              " 'others',\n",
              " 'trying',\n",
              " 'tag',\n",
              " 'copyright',\n",
              " 'must',\n",
              " 'understand',\n",
              " 'above',\n",
              " 'few',\n",
              " 'anyone',\n",
              " 'speedy',\n",
              " 'last',\n",
              " 'issue',\n",
              " 'give',\n",
              " 'questions',\n",
              " 'agree',\n",
              " 'rather',\n",
              " 'years',\n",
              " 'let',\n",
              " '2',\n",
              " 'different',\n",
              " 'editor',\n",
              " 'long',\n",
              " 'reliable',\n",
              " 'making',\n",
              " 'world',\n",
              " 'come',\n",
              " 'sorry',\n",
              " 'isnt',\n",
              " 'reference',\n",
              " 'mean',\n",
              " 'continue',\n",
              " 'try',\n",
              " 'references',\n",
              " 'found',\n",
              " 'doing',\n",
              " 'text',\n",
              " 'great',\n",
              " 'leave',\n",
              " 'says',\n",
              " 'got',\n",
              " 'probably',\n",
              " 'english',\n",
              " 'original',\n",
              " 'every',\n",
              " '1',\n",
              " 'simply',\n",
              " 'word',\n",
              " 'users',\n",
              " 'fair',\n",
              " 'hello',\n",
              " 'either',\n",
              " 'check',\n",
              " 'least',\n",
              " 'adding',\n",
              " 'ip',\n",
              " 'show',\n",
              " 'site',\n",
              " 'state',\n",
              " 'else',\n",
              " 'delete',\n",
              " 'consensus',\n",
              " 'enough',\n",
              " 'request',\n",
              " 'far',\n",
              " 'opinion',\n",
              " 'created',\n",
              " 'around',\n",
              " 'life',\n",
              " 'day',\n",
              " 'between',\n",
              " 'through',\n",
              " 'example',\n",
              " 'view',\n",
              " 'yes',\n",
              " 'reverted',\n",
              " 'yet',\n",
              " 'etc',\n",
              " 'id',\n",
              " 'matter',\n",
              " 'shit',\n",
              " 'u',\n",
              " 'war',\n",
              " 'notable',\n",
              " 'contributions',\n",
              " 'given',\n",
              " 'thought',\n",
              " 'material',\n",
              " 'book',\n",
              " 'admin',\n",
              " 'write',\n",
              " 'post',\n",
              " 'down',\n",
              " 'account',\n",
              " 'clearly',\n",
              " 'having',\n",
              " 'encyclopedia',\n",
              " 'lot',\n",
              " 'support',\n",
              " 'real',\n",
              " 'bad',\n",
              " 'message',\n",
              " 'needs',\n",
              " 'images',\n",
              " 'tell',\n",
              " 'seem',\n",
              " 'called',\n",
              " 'maybe',\n",
              " 'evidence',\n",
              " 'instead',\n",
              " 'ever',\n",
              " '3',\n",
              " 'correct',\n",
              " 'saying',\n",
              " 'clear',\n",
              " 'always',\n",
              " 'number',\n",
              " 'important',\n",
              " 'further',\n",
              " 'quite',\n",
              " 'perhaps',\n",
              " 'old',\n",
              " '—',\n",
              " 'true',\n",
              " 'until',\n",
              " 'hate',\n",
              " 'states',\n",
              " 'whether',\n",
              " 'consider',\n",
              " 'written',\n",
              " 'claim',\n",
              " 'language',\n",
              " 'media',\n",
              " 'bit',\n",
              " 'once',\n",
              " 'guidelines',\n",
              " 'term',\n",
              " 'criteria',\n",
              " 'research',\n",
              " 'nigger',\n",
              " 'version',\n",
              " 'times',\n",
              " 'website',\n",
              " 'getting',\n",
              " 'fucking',\n",
              " 'theres',\n",
              " 'review',\n",
              " 'mention',\n",
              " 'pov',\n",
              " 'oh',\n",
              " 'makes',\n",
              " 'several',\n",
              " 'revert',\n",
              " 'considered',\n",
              " 'changes',\n",
              " 'cannot',\n",
              " 'words',\n",
              " 'idea',\n",
              " 'title',\n",
              " 'suck',\n",
              " 'address',\n",
              " 'notice',\n",
              " 'based',\n",
              " 'top',\n",
              " 'following',\n",
              " 'current',\n",
              " 'each',\n",
              " 'listed',\n",
              " 'means',\n",
              " 'possible',\n",
              " 'group',\n",
              " 'facts',\n",
              " 'regarding',\n",
              " 'care',\n",
              " 'rules',\n",
              " 'second',\n",
              " 'main',\n",
              " 'template',\n",
              " 'mentioned',\n",
              " 'general',\n",
              " 'year',\n",
              " 'attack',\n",
              " 'kind',\n",
              " 'whole',\n",
              " 'course',\n",
              " 'statement',\n",
              " 'left',\n",
              " 'hey',\n",
              " 'date',\n",
              " 'include',\n",
              " 'seen',\n",
              " 'three',\n",
              " 'issues',\n",
              " 'start',\n",
              " 'ass',\n",
              " 'ok',\n",
              " 'end',\n",
              " 'wikipedias',\n",
              " 'call',\n",
              " 'less',\n",
              " 'topic',\n",
              " 'gay',\n",
              " 'suggest',\n",
              " 'man',\n",
              " 'including',\n",
              " 'happy',\n",
              " 'sense',\n",
              " 'provide',\n",
              " 'create',\n",
              " 'big',\n",
              " 'days',\n",
              " 'myself',\n",
              " 'american',\n",
              " 'redirect',\n",
              " 'known',\n",
              " 'sentence',\n",
              " 'move',\n",
              " 'appropriate',\n",
              " 'changed',\n",
              " 'love',\n",
              " 'notability',\n",
              " 'explain',\n",
              " 'started',\n",
              " 'included',\n",
              " 'removing',\n",
              " 'project',\n",
              " 'anyway',\n",
              " 'info',\n",
              " 'mind',\n",
              " 'school',\n",
              " '2005',\n",
              " 'next',\n",
              " 'looking',\n",
              " 'although',\n",
              " 'picture',\n",
              " 'relevant',\n",
              " 'four',\n",
              " 'die',\n",
              " 'sign',\n",
              " 'answer',\n",
              " 'style',\n",
              " 'away',\n",
              " 'per',\n",
              " 'order',\n",
              " 'warning',\n",
              " 'wont',\n",
              " 'recent',\n",
              " 'youve',\n",
              " 'interest',\n",
              " 'community',\n",
              " 'summary',\n",
              " 'later',\n",
              " 'lol',\n",
              " 'claims',\n",
              " 'currently',\n",
              " 'discuss',\n",
              " 'interested',\n",
              " 'policies',\n",
              " 'attacks',\n",
              " 'especially',\n",
              " 'wish',\n",
              " 'wrote',\n",
              " 'able',\n",
              " 'specific',\n",
              " 'public',\n",
              " 'taken',\n",
              " 'writing',\n",
              " 'neutral',\n",
              " 'full',\n",
              " 'names',\n",
              " 'within',\n",
              " '4',\n",
              " 'position',\n",
              " 'related',\n",
              " 'below',\n",
              " 'line',\n",
              " 'wanted',\n",
              " 'during',\n",
              " 'appears',\n",
              " 'stuff',\n",
              " 'certainly',\n",
              " 'official',\n",
              " 'nice',\n",
              " 'itself',\n",
              " 'faith',\n",
              " 'everyone',\n",
              " 'wasnt',\n",
              " 'live',\n",
              " 'report',\n",
              " 'completely',\n",
              " 'according',\n",
              " 'unless',\n",
              " 'common',\n",
              " 'pretty',\n",
              " 'country',\n",
              " 'everything',\n",
              " 'looks',\n",
              " 'due',\n",
              " 'single',\n",
              " 'hes',\n",
              " 'process',\n",
              " 'contribs',\n",
              " 'news',\n",
              " 'involved',\n",
              " 'god',\n",
              " 'fat',\n",
              " 'therefore',\n",
              " 'obviously',\n",
              " 'remember',\n",
              " 'lead',\n",
              " 'hard',\n",
              " 'admins',\n",
              " 'came',\n",
              " 'edited',\n",
              " 'web',\n",
              " 'stay',\n",
              " 'learn',\n",
              " 'response',\n",
              " 'future',\n",
              " 'past',\n",
              " 'asked',\n",
              " 'truth',\n",
              " 'reading',\n",
              " 'power',\n",
              " '2006',\n",
              " 'stupid',\n",
              " 'entry',\n",
              " 'quote',\n",
              " 'posted',\n",
              " 'nor',\n",
              " 'talking',\n",
              " 'placed',\n",
              " '5',\n",
              " 'ago',\n",
              " 'similar',\n",
              " 'email',\n",
              " 'game',\n",
              " 'published',\n",
              " 'exactly',\n",
              " 'today',\n",
              " 'reasons',\n",
              " 'paragraph',\n",
              " 'faggot',\n",
              " 'city',\n",
              " 'argument',\n",
              " 'whatever',\n",
              " 'system',\n",
              " 'working',\n",
              " 'false',\n",
              " 'sandbox',\n",
              " 'moron',\n",
              " 'political',\n",
              " 'noticed',\n",
              " 'useful',\n",
              " 'havent',\n",
              " 'guy',\n",
              " 'high',\n",
              " 'regards',\n",
              " 'united',\n",
              " 'guess',\n",
              " 'appreciate',\n",
              " 'particular',\n",
              " 'deleting',\n",
              " 'form',\n",
              " 'books',\n",
              " 'government',\n",
              " 'dispute',\n",
              " 'five',\n",
              " 'british',\n",
              " 'reverting',\n",
              " 'major',\n",
              " 'problems',\n",
              " 'national',\n",
              " 'party',\n",
              " 'provided',\n",
              " 'often',\n",
              " 'ones',\n",
              " 'become',\n",
              " 'lets',\n",
              " 'tried',\n",
              " 'side',\n",
              " 'administrator',\n",
              " 'along',\n",
              " 'reply',\n",
              " 'almost',\n",
              " 'needed',\n",
              " 'stated',\n",
              " 'rule',\n",
              " 'took',\n",
              " 'search',\n",
              " 'knowledge',\n",
              " 'banned',\n",
              " 'cheers',\n",
              " 'taking',\n",
              " 'vandalize',\n",
              " '–',\n",
              " 'certain',\n",
              " '2007',\n",
              " 'username',\n",
              " 'fine',\n",
              " 'status',\n",
              " 'law',\n",
              " 'points',\n",
              " 'company',\n",
              " 'otherwise',\n",
              " 'uploaded',\n",
              " 'terms',\n",
              " 'explanation',\n",
              " 'generally',\n",
              " 'sort',\n",
              " 'entire',\n",
              " 'shows',\n",
              " 'description',\n",
              " 'whats',\n",
              " 'recently',\n",
              " 'follow',\n",
              " 'guys',\n",
              " '2008',\n",
              " 'likely',\n",
              " 'film',\n",
              " 'present',\n",
              " 'aware',\n",
              " 'saw',\n",
              " 'definition',\n",
              " 'cited',\n",
              " 'alone',\n",
              " 'google',\n",
              " 'music',\n",
              " 'soon',\n",
              " 'indeed',\n",
              " 'decide',\n",
              " 'ban',\n",
              " 'wp',\n",
              " 'appear',\n",
              " 'views',\n",
              " 'week',\n",
              " 'open',\n",
              " 'citation',\n",
              " 'contributing',\n",
              " 'actual',\n",
              " 'set',\n",
              " 'interesting',\n",
              " 'piece',\n",
              " 'c',\n",
              " 'short',\n",
              " 'white',\n",
              " 'told',\n",
              " 'theory',\n",
              " 'area',\n",
              " 'improve',\n",
              " 'external',\n",
              " 'small',\n",
              " 'story',\n",
              " 'contact',\n",
              " 'simple',\n",
              " '2004',\n",
              " 'various',\n",
              " 'allowed',\n",
              " 'moved',\n",
              " 'test',\n",
              " 'internet',\n",
              " 'obvious',\n",
              " 'family',\n",
              " 'band',\n",
              " 'attention',\n",
              " 'arent',\n",
              " 'proposed',\n",
              " 'jew',\n",
              " 'themselves',\n",
              " 'members',\n",
              " 'wouldnt',\n",
              " 'result',\n",
              " 'disagree',\n",
              " 'thus',\n",
              " 'cunt',\n",
              " 'went',\n",
              " 'type',\n",
              " 'sites',\n",
              " 'ie',\n",
              " 'context',\n",
              " 'mr',\n",
              " 'previous',\n",
              " 'nonsense',\n",
              " 'actions',\n",
              " 'tags',\n",
              " 'cite',\n",
              " 'works',\n",
              " '10',\n",
              " 'citations',\n",
              " 'jews',\n",
              " 'university',\n",
              " 're',\n",
              " 'enjoy',\n",
              " 'conflict',\n",
              " 'hours',\n",
              " 'shouldnt',\n",
              " 'proper',\n",
              " 'bias',\n",
              " 'category',\n",
              " 'job',\n",
              " 'longer',\n",
              " 'file',\n",
              " 'together',\n",
              " 'hell',\n",
              " 'sourced',\n",
              " 'sucks',\n",
              " 'addition',\n",
              " 'happened',\n",
              " 'avoid',\n",
              " 'automatically',\n",
              " 'author',\n",
              " 'valid',\n",
              " 'black',\n",
              " 'creating',\n",
              " 'deal',\n",
              " 'worked',\n",
              " 'npov',\n",
              " 'goes',\n",
              " 'himself',\n",
              " 'seriously',\n",
              " 'john',\n",
              " 'death',\n",
              " 'proof',\n",
              " 'respect',\n",
              " 'bitch',\n",
              " 'science',\n",
              " 'human',\n",
              " 'biased',\n",
              " 'comes',\n",
              " 'helpful',\n",
              " 'large',\n",
              " 'accepted',\n",
              " 'available',\n",
              " 'exist',\n",
              " 'series',\n",
              " 'tildes',\n",
              " 'opinions',\n",
              " 'hand',\n",
              " '6',\n",
              " 'indicate',\n",
              " 'sections',\n",
              " 'rights',\n",
              " 'necessary',\n",
              " 'act',\n",
              " 'meaning',\n",
              " 'attempt',\n",
              " 'accept',\n",
              " 'personally',\n",
              " 'statements',\n",
              " 'violation',\n",
              " 'months',\n",
              " 'criticism',\n",
              " 'accurate',\n",
              " 'action',\n",
              " 'usually',\n",
              " 'unblock',\n",
              " 'german',\n",
              " 'pig',\n",
              " 'cause',\n",
              " 'yeah',\n",
              " 'living',\n",
              " 'copy',\n",
              " 'debate',\n",
              " 'upon',\n",
              " 'assume',\n",
              " 'july',\n",
              " 'calling',\n",
              " 'standard',\n",
              " 'video',\n",
              " 'play',\n",
              " 'rest',\n",
              " 'tagged',\n",
              " 'doubt',\n",
              " 'sex',\n",
              " 'multiple',\n",
              " 'theyre',\n",
              " 'historical',\n",
              " 'serious',\n",
              " 'details',\n",
              " 'dick',\n",
              " 'youll',\n",
              " 'separate',\n",
              " 'manual',\n",
              " 'record',\n",
              " 'blocking',\n",
              " 'afd',\n",
              " 'explaining',\n",
              " 'situation',\n",
              " 'refer',\n",
              " 'wikiproject',\n",
              " 'heard',\n",
              " 'online',\n",
              " 'level',\n",
              " 'fix',\n",
              " 'asking',\n",
              " '7',\n",
              " 'complete',\n",
              " 'speak',\n",
              " 'lack',\n",
              " 'messages',\n",
              " 'none',\n",
              " 'prove',\n",
              " 'third',\n",
              " 'subjects',\n",
              " 'church',\n",
              " 'apparently',\n",
              " '2009',\n",
              " 'south',\n",
              " 'rationale',\n",
              " 'bullshit',\n",
              " 'data',\n",
              " 'directly',\n",
              " 'august',\n",
              " 'period',\n",
              " 'legal',\n",
              " 'behavior',\n",
              " 'difference',\n",
              " 'contribute',\n",
              " 'greek',\n",
              " 'huge',\n",
              " 'gets',\n",
              " 'wikipedian',\n",
              " 'couple',\n",
              " 'supposed',\n",
              " 'among',\n",
              " 'early',\n",
              " 'except',\n",
              " 'march',\n",
              " 'close',\n",
              " 'quality',\n",
              " 'space',\n",
              " 'meant',\n",
              " 'countries',\n",
              " 'run',\n",
              " 'team',\n",
              " 'uses',\n",
              " 'military',\n",
              " 'b',\n",
              " 'changing',\n",
              " 'existing',\n",
              " 'specifically',\n",
              " 'significant',\n",
              " '2010',\n",
              " 'pillars',\n",
              " 'fish',\n",
              " 'incorrect',\n",
              " 'culture',\n",
              " 'described',\n",
              " 'produce',\n",
              " 'jewish',\n",
              " '24',\n",
              " 'uk',\n",
              " 'disruptive',\n",
              " 'd',\n",
              " 'field',\n",
              " 'error',\n",
              " 'india',\n",
              " 'head',\n",
              " 'primary',\n",
              " 'friend',\n",
              " 'earlier',\n",
              " 'sometimes',\n",
              " 'outside',\n",
              " '20',\n",
              " 'purpose',\n",
              " 'administrators',\n",
              " 'modern',\n",
              " 'photo',\n",
              " 'table',\n",
              " 'particularly',\n",
              " 't',\n",
              " 'release',\n",
              " 'gave',\n",
              " 'box',\n",
              " 'cases',\n",
              " 'inclusion',\n",
              " 'born',\n",
              " 'pictures',\n",
              " 'readers',\n",
              " 'june',\n",
              " 'character',\n",
              " 'vote',\n",
              " 'okay',\n",
              " 'groups',\n",
              " 'anonymous',\n",
              " 'abuse',\n",
              " 'arguments',\n",
              " 'business',\n",
              " 'shall',\n",
              " 'sock',\n",
              " 'tutorial',\n",
              " 'january',\n",
              " 'friends',\n",
              " 'numbers',\n",
              " 'control',\n",
              " 'thinking',\n",
              " 'member',\n",
              " 'linked',\n",
              " 'happen',\n",
              " 'reported',\n",
              " 'contest',\n",
              " 'coming',\n",
              " 'takes',\n",
              " 'concerns',\n",
              " 'allow',\n",
              " 'wait',\n",
              " 'majority',\n",
              " 'giving',\n",
              " '8',\n",
              " 'bring',\n",
              " 'eg',\n",
              " 'worth',\n",
              " 'kill',\n",
              " 'totally',\n",
              " 'red',\n",
              " 'force',\n",
              " 'decided',\n",
              " 'discussed',\n",
              " 'house',\n",
              " 'finally',\n",
              " 'absolutely',\n",
              " 'putting',\n",
              " 'scientific',\n",
              " 'respond',\n",
              " 'mistake',\n",
              " 'decision',\n",
              " 'de',\n",
              " 'lost',\n",
              " 'entirely',\n",
              " '100',\n",
              " 'towards',\n",
              " 'merely',\n",
              " 'home',\n",
              " 'neither',\n",
              " 'dear',\n",
              " 'independent',\n",
              " 'international',\n",
              " 'song',\n",
              " 'balls',\n",
              " 'wants',\n",
              " 'possibly',\n",
              " 'unsigned',\n",
              " 'million',\n",
              " 'irrelevant',\n",
              " 'standards',\n",
              " 'april',\n",
              " '12',\n",
              " 'press',\n",
              " 'figure',\n",
              " 'organization',\n",
              " 'looked',\n",
              " 'inappropriate',\n",
              " 'chance',\n",
              " 'posting',\n",
              " 'population',\n",
              " 'advice',\n",
              " 'posts',\n",
              " 'north',\n",
              " 'events',\n",
              " 'unfortunately',\n",
              " 'named',\n",
              " 'album',\n",
              " ...]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrX4Qr1y-81",
        "outputId": "d69202ce-1def-4f33-9222-65346f7cb74e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([    1,   523,     1,    29, 37427])>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example of text tokenizing\n",
        "vectorizer(\"Helloo everyone whassup my homies\")[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E08RKn6EznLd"
      },
      "outputs": [],
      "source": [
        "# tokenizing the dataset\n",
        "vectorizedText = vectorizer(x.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEtV-tM20HQd",
        "outputId": "29971675-670c-4f76-b23d-c8bcd987442d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
              "array([[  645,    76,     2, ...,     0,     0,     0],\n",
              "       [    1,    54,  2489, ...,     0,     0,     0],\n",
              "       [  425,   441,    70, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [32445,  7392,   383, ...,     0,     0,     0],\n",
              "       [    5,    12,   534, ...,     0,     0,     0],\n",
              "       [    5,     8,   130, ...,     0,     0,     0]])>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizedText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "-mDcQyB5ZAB9",
        "outputId": "a5555c24-5a53-4aee-a76c-5aa5d616a4f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.dataset_ops.DatasetV2.list_files</b><br/>def list_files(file_pattern, shuffle=None, seed=None, name=None) -&gt; &#x27;DatasetV2&#x27;</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py</a>A dataset of all files matching one or more glob patterns.\n",
              "\n",
              "The `file_pattern` argument should be a small number of glob patterns.\n",
              "If your filenames have already been globbed, use\n",
              "`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
              "filename with `list_files` may result in poor performance with remote\n",
              "storage systems.\n",
              "\n",
              "Note: The default behavior of this method is to return filenames in\n",
              "a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
              "to get results in a deterministic order.\n",
              "\n",
              "Example:\n",
              "  If we had the following files on our filesystem:\n",
              "\n",
              "    - /path/to/dir/a.txt\n",
              "    - /path/to/dir/b.py\n",
              "    - /path/to/dir/c.py\n",
              "\n",
              "  If we pass &quot;/path/to/dir/*.py&quot; as the directory, the dataset\n",
              "  would produce:\n",
              "\n",
              "    - /path/to/dir/b.py\n",
              "    - /path/to/dir/c.py\n",
              "\n",
              "Args:\n",
              "  file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
              "    (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
              "    pattern(s) that will be matched.\n",
              "  shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
              "    Defaults to `True`.\n",
              "  seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
              "    seed that will be used to create the distribution. See\n",
              "    `tf.random.set_seed` for behavior.\n",
              "  name: Optional. A name for the tf.data operations used by `list_files`.\n",
              "\n",
              "Returns:\n",
              " Dataset: A `Dataset` of strings corresponding to file names.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1271);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "<function tensorflow.python.data.ops.dataset_ops.DatasetV2.list_files(file_pattern, shuffle=None, seed=None, name=None) -> 'DatasetV2'>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.data.Dataset.list_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQh8cLUOWqiw"
      },
      "outputs": [],
      "source": [
        "#MCSHBAP - map , cache ,shuffle ,batch ,prefetch from tenserflow slices\n",
        "# batch the data\n",
        "dataset = tf.data.Dataset.from_tensor_slices((vectorizedText,y))# passing the input data and the corresponding labels\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(160000)\n",
        "dataset = dataset.batch(16)\n",
        "dataset = dataset.prefetch(8)# helps prevent bottlenecks\n",
        "#Prefetching enables the system to load the next batch of data while the current batch is being processed by the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r10p_d_boi2",
        "outputId": "3207ae7a-2e8a-40ef-be1c-491d24191d4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[2149, 2149, 2149, ...,    0,    0,    0],\n",
              "        [  19,    7,  988, ...,    0,    0,    0],\n",
              "        [ 860,    4,  640, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 119,    3, 1626, ...,    0,    0,    0],\n",
              "        [ 104,   76,  215, ...,    0,    0,    0],\n",
              "        [  46,  168,   23, ...,    0,    0,    0]]),\n",
              " array([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.as_numpy_iterator().next()\n",
        "# load a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdkVotFscgle"
      },
      "outputs": [],
      "source": [
        "batch_x , batch_y = dataset.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrv_EaLScx6N",
        "outputId": "15ee3acf-eec3-4130-9218-ff825d49de1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16, 6)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_x.shape # inputs\n",
        "batch_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh0RsceXdLiG"
      },
      "outputs": [],
      "source": [
        "train = dataset.take(int(len(dataset )*0.7))\n",
        "val = dataset.skip(int(len(dataset)*0.7)).take(int(len(dataset)*0.2))\n",
        "test = dataset.skip(int(len(dataset)*0.9)).take(int(len(dataset)*0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLS8XvWFfcn1"
      },
      "outputs": [],
      "source": [
        "train_generator = train.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD3r-bpff2fq",
        "outputId": "86875631-6fc0-4d8a-df06-e04b42c65e39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[    8,  5332,    22, ...,     0,     0,     0],\n",
              "        [    7,    19,    15, ...,     0,     0,     0],\n",
              "        [   21,    42,     6, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [   88,  1189, 98077, ...,     0,     0,     0],\n",
              "        [  191,   288,     1, ...,     0,     0,     0],\n",
              "        [47884, 45558, 50792, ...,     0,     0,     0]]),\n",
              " array([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZzJZUPwgg75"
      },
      "source": [
        "create sequencial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp1tdA27geH7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM , Dropout, Bidirectional,Dense ,Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MTk19oyiDeP"
      },
      "outputs": [],
      "source": [
        "model  = Sequential()# instantiate the sequential API so after that we can add the layers to the model?\n",
        "# create the embedding layer\n",
        "\n",
        "model.add(Embedding(MAX_WORDS+1,32)) # +1 is for the unknown words\n",
        "# bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(64 , activation = \"tanh\")))\n",
        "# dense layers\n",
        "model.add(Dense(128,activation = \"relu\"))\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dense(138, activation = \"relu\"))\n",
        "model.add(Dropout(0.3))  # Drop 30% of neurons randomly. Dropout Layer (Before Final Output)\n",
        "\n",
        "# converting the previous dense layer output to values between 0 and 1\n",
        "model.add(Dense(6 , activation = \"sigmoid\"))\n",
        "# dense layers connect each input to each output within its layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "7w7e7M21wJPr",
        "outputId": "6a300785-f38b-4b8c-f7f8-63b8d717420d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CCoMXNU7EAf"
      },
      "source": [
        "Compile the model with F1 score , accuracy, recall ,precision Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-SdigO42zqb"
      },
      "outputs": [],
      "source": [
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"f1_score\", **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * (precision * recall) / (precision + recall + epsilon())\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EusdqYpgrfeQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.backend import epsilon\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\n",
        "        \"accuracy\",\n",
        "        Precision(name=\"precision\"),\n",
        "        Recall(name=\"recall\"),\n",
        "        AUC(name=\"auc\"),\n",
        "        F1Score()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR-Y4U7K7XG8"
      },
      "source": [
        "MLOPs integrated training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MQmibRQBRyU",
        "outputId": "b0b28de8-6b69-43dc-a5b0-281837420ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9910 - auc: 0.9849 - f1_score: 0.7567 - loss: 0.0447 - precision: 0.8301 - recall: 0.6953 - val_accuracy: 0.9937 - val_auc: 0.9913 - val_f1_score: 0.7823 - val_loss: 0.0396 - val_precision: 0.8483 - val_recall: 0.7259\n",
            "Epoch 2/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9916 - auc: 0.9882 - f1_score: 0.7786 - loss: 0.0388 - precision: 0.8391 - recall: 0.7263 - val_accuracy: 0.9892 - val_auc: 0.9908 - val_f1_score: 0.8000 - val_loss: 0.0350 - val_precision: 0.8684 - val_recall: 0.7415\n",
            "Epoch 3/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9933 - auc: 0.9912 - f1_score: 0.8024 - loss: 0.0349 - precision: 0.8509 - recall: 0.7592 - val_accuracy: 0.9946 - val_auc: 0.9935 - val_f1_score: 0.8105 - val_loss: 0.0310 - val_precision: 0.9038 - val_recall: 0.7347\n",
            "Epoch 4/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9909 - auc: 0.9922 - f1_score: 0.8205 - loss: 0.0326 - precision: 0.8578 - recall: 0.7863 - val_accuracy: 0.9941 - val_auc: 0.9944 - val_f1_score: 0.8369 - val_loss: 0.0303 - val_precision: 0.8451 - val_recall: 0.8289\n",
            "Epoch 5/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9857 - auc: 0.9936 - f1_score: 0.8408 - loss: 0.0296 - precision: 0.8663 - recall: 0.8169 - val_accuracy: 0.9939 - val_auc: 0.9954 - val_f1_score: 0.8530 - val_loss: 0.0267 - val_precision: 0.8964 - val_recall: 0.8135\n",
            "Epoch 6/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9910 - auc: 0.9942 - f1_score: 0.8553 - loss: 0.0278 - precision: 0.8739 - recall: 0.8375 - val_accuracy: 0.9945 - val_auc: 0.9963 - val_f1_score: 0.8720 - val_loss: 0.0246 - val_precision: 0.8756 - val_recall: 0.8684\n",
            "Epoch 7/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9835 - auc: 0.9951 - f1_score: 0.8724 - loss: 0.0250 - precision: 0.8877 - recall: 0.8578 - val_accuracy: 0.9949 - val_auc: 0.9964 - val_f1_score: 0.8899 - val_loss: 0.0227 - val_precision: 0.8898 - val_recall: 0.8900\n",
            "Epoch 8/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 106ms/step - accuracy: 0.9751 - auc: 0.9953 - f1_score: 0.8806 - loss: 0.0232 - precision: 0.8930 - recall: 0.8686 - val_accuracy: 0.9932 - val_auc: 0.9976 - val_f1_score: 0.8997 - val_loss: 0.0210 - val_precision: 0.8873 - val_recall: 0.9124\n",
            "Epoch 9/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 106ms/step - accuracy: 0.9271 - auc: 0.9957 - f1_score: 0.8936 - loss: 0.0213 - precision: 0.9043 - recall: 0.8832 - val_accuracy: 0.8615 - val_auc: 0.9969 - val_f1_score: 0.8911 - val_loss: 0.0200 - val_precision: 0.9128 - val_recall: 0.8704\n",
            "Epoch 10/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.8237 - auc: 0.9964 - f1_score: 0.8989 - loss: 0.0202 - precision: 0.9079 - recall: 0.8902 - val_accuracy: 0.9869 - val_auc: 0.9966 - val_f1_score: 0.9070 - val_loss: 0.0184 - val_precision: 0.9264 - val_recall: 0.8884\n",
            "Epoch 11/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9050 - auc: 0.9963 - f1_score: 0.9083 - loss: 0.0188 - precision: 0.9123 - recall: 0.9043 - val_accuracy: 0.9938 - val_auc: 0.9972 - val_f1_score: 0.9184 - val_loss: 0.0165 - val_precision: 0.9448 - val_recall: 0.8935\n",
            "Epoch 12/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9769 - auc: 0.9969 - f1_score: 0.9157 - loss: 0.0180 - precision: 0.9196 - recall: 0.9120 - val_accuracy: 0.9909 - val_auc: 0.9982 - val_f1_score: 0.9260 - val_loss: 0.0151 - val_precision: 0.9358 - val_recall: 0.9163\n",
            "Epoch 13/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9638 - auc: 0.9976 - f1_score: 0.9252 - loss: 0.0157 - precision: 0.9294 - recall: 0.9211 - val_accuracy: 0.9880 - val_auc: 0.9986 - val_f1_score: 0.9272 - val_loss: 0.0144 - val_precision: 0.9141 - val_recall: 0.9407\n",
            "Epoch 14/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9739 - auc: 0.9975 - f1_score: 0.9259 - loss: 0.0146 - precision: 0.9263 - recall: 0.9254 - val_accuracy: 0.9897 - val_auc: 0.9984 - val_f1_score: 0.9346 - val_loss: 0.0129 - val_precision: 0.9376 - val_recall: 0.9315\n",
            "Epoch 15/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9573 - auc: 0.9973 - f1_score: 0.9334 - loss: 0.0138 - precision: 0.9342 - recall: 0.9326 - val_accuracy: 0.9907 - val_auc: 0.9985 - val_f1_score: 0.9400 - val_loss: 0.0116 - val_precision: 0.9463 - val_recall: 0.9338\n",
            "Epoch 16/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.8470 - auc: 0.9976 - f1_score: 0.9323 - loss: 0.0135 - precision: 0.9332 - recall: 0.9314 - val_accuracy: 0.9783 - val_auc: 0.9985 - val_f1_score: 0.9390 - val_loss: 0.0127 - val_precision: 0.9276 - val_recall: 0.9506\n",
            "Epoch 17/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.8815 - auc: 0.9982 - f1_score: 0.9381 - loss: 0.0124 - precision: 0.9391 - recall: 0.9371 - val_accuracy: 0.8999 - val_auc: 0.9986 - val_f1_score: 0.9405 - val_loss: 0.0108 - val_precision: 0.9363 - val_recall: 0.9448\n",
            "Epoch 18/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.8958 - auc: 0.9978 - f1_score: 0.9394 - loss: 0.0120 - precision: 0.9405 - recall: 0.9383 - val_accuracy: 0.9911 - val_auc: 0.9985 - val_f1_score: 0.9467 - val_loss: 0.0107 - val_precision: 0.9455 - val_recall: 0.9479\n",
            "Epoch 19/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9631 - auc: 0.9983 - f1_score: 0.9454 - loss: 0.0108 - precision: 0.9460 - recall: 0.9449 - val_accuracy: 0.9864 - val_auc: 0.9983 - val_f1_score: 0.9498 - val_loss: 0.0098 - val_precision: 0.9451 - val_recall: 0.9545\n",
            "Epoch 20/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9117 - auc: 0.9985 - f1_score: 0.9471 - loss: 0.0104 - precision: 0.9449 - recall: 0.9492 - val_accuracy: 0.9875 - val_auc: 0.9987 - val_f1_score: 0.9531 - val_loss: 0.0090 - val_precision: 0.9552 - val_recall: 0.9510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow & W&B logging complete with accuracy: 0.93\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>epoch/accuracy</td><td>▇███████▇▄▄▇▇▇▇▇▁▄▅▇▆</td></tr><tr><td>epoch/auc</td><td>▁▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch/f1_score</td><td>▁▃▄▄▅▅▆▆▆▇▇▇▇▇███████</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch/precision</td><td>▁▃▃▄▄▄▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch/recall</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>epoch/val_accuracy</td><td>█████████▁██████▇▃███</td></tr><tr><td>epoch/val_auc</td><td>▁▄▄▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/val_f1_score</td><td>▁▂▃▃▄▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>epoch/val_precision</td><td>▁▂▃▅▁▄▃▄▄▅▆▇▇▆▇▇▆▇▇▇█</td></tr><tr><td>epoch/val_recall</td><td>▁▃▃▃▅▅▆▆▇▆▆▇▇█▇██████</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.92711</td></tr><tr><td>epoch/accuracy</td><td>0.92711</td></tr><tr><td>epoch/auc</td><td>0.99832</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/f1_score</td><td>0.94539</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01075</td></tr><tr><td>epoch/precision</td><td>0.9446</td></tr><tr><td>epoch/recall</td><td>0.94618</td></tr><tr><td>epoch/val_accuracy</td><td>0.98753</td></tr><tr><td>epoch/val_auc</td><td>0.99868</td></tr><tr><td>epoch/val_f1_score</td><td>0.95305</td></tr><tr><td>epoch/val_loss</td><td>0.00898</td></tr><tr><td>epoch/val_precision</td><td>0.95516</td></tr><tr><td>epoch/val_recall</td><td>0.95095</td></tr><tr><td>loss</td><td>0.01075</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-firebrand-1</strong> at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/i2zqmk3c' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/i2zqmk3c</a><br> View project at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a><br>Synced 5 W&B file(s), 2 media file(s), 40 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250225_015324-i2zqmk3c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# MLflow Experiment Setup\n",
        "mlflow.set_experiment(\"toxicity_detection\")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Train Model with Logging\n",
        "    history = model.fit(\n",
        "        train,\n",
        "        epochs=20,\n",
        "        validation_data=val,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor=\"val_loss\", patience=3),\n",
        "            WandbMetricsLogger(),  # Logs to W&B\n",
        "            WandbModelCheckpoint(filepath=\"wandb_model.keras\", save_best_only=True)  # ✅ Saves best model\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Retrieve Final Metrics\n",
        "    final_accuracy = history.history[\"accuracy\"][-1]\n",
        "    final_loss = history.history[\"loss\"][-1]\n",
        "\n",
        "    # Log Metrics to MLflow\n",
        "    mlflow.log_param(\"epochs\", 20)\n",
        "    mlflow.log_metric(\"accuracy\", final_accuracy)\n",
        "    mlflow.log_metric(\"loss\", final_loss)\n",
        "\n",
        "    # Save and Log Model\n",
        "    model.save(\"toxicity_model.h5\")\n",
        "    mlflow.log_artifact(\"toxicity_model.h5\")\n",
        "\n",
        "    # Log Epoch-wise Metrics\n",
        "    for epoch in range(len(history.history[\"loss\"])):\n",
        "        mlflow.log_metric(\"train_loss\", history.history[\"loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", history.history[\"val_loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"train_f1\", history.history[\"f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_f1\", history.history[\"val_f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"train_accuracy\", history.history[\"accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", history.history[\"val_accuracy\"][epoch], step=epoch)\n",
        "\n",
        "    # Log Metrics to W&B\n",
        "    wandb.log({\n",
        "        \"accuracy\": final_accuracy,\n",
        "        \"loss\": final_loss\n",
        "    })\n",
        "\n",
        "    # Plot Loss & F1-Score Curves\n",
        "    def plot_and_log_metric(metric_name, title):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(history.history[metric_name], label=\"Train \" + metric_name.capitalize())\n",
        "        plt.plot(history.history[\"val_\" + metric_name], label=\"Validation \" + metric_name.capitalize())\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(metric_name.capitalize())\n",
        "        plt.legend()\n",
        "        plt.title(title)\n",
        "        plt.savefig(f\"{metric_name}_plot.png\")\n",
        "        mlflow.log_artifact(f\"{metric_name}_plot.png\")  # Log to MLflow\n",
        "        wandb.log({f\"{metric_name}_plot\": wandb.Image(f\"{metric_name}_plot.png\")})  # Log to W&B\n",
        "        plt.close()\n",
        "\n",
        "    # Log Loss & F1-score Plots\n",
        "    plot_and_log_metric(\"loss\", \"Loss over Epochs\")\n",
        "    plot_and_log_metric(\"f1_score\", \"F1 Score over Epochs\")\n",
        "\n",
        "    print(f\"MLflow & W&B logging complete with accuracy: {final_accuracy:.2f}\")\n",
        "\n",
        "# Finish W&B Run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F84IdY8A1x2",
        "outputId": "c6aee0a9-db16-4362-c3fd-6c741527589b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['loss', '_runtime', 'epoch/val_precision', 'epoch/accuracy',\n",
            "       'epoch/f1_score', 'epoch/val_recall', 'accuracy', 'epoch/auc',\n",
            "       'epoch/learning_rate', '_step', 'epoch/precision', 'epoch/loss',\n",
            "       'epoch/recall', 'epoch/val_auc', 'loss_plot', 'epoch/val_f1_score',\n",
            "       '_timestamp', 'epoch/val_accuracy', 'epoch/epoch', 'f1_score_plot',\n",
            "       'epoch/val_loss'],\n",
            "      dtype='object')\n",
            "Plots regenerated and logged successfully.\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "\n",
        "# Initialize W&B Run (Load previous run)\n",
        "api = wandb.Api()\n",
        "run = api.run(\"toxicity-detection/i2zqmk3c\")\n",
        "history = run.history()\n",
        "print(history.columns)  # Shows all available metric names\n",
        "\n",
        "# Convert W&B history to match your format\n",
        "history_dict = {\n",
        "    \"loss\": history[\"epoch/loss\"].tolist(),\n",
        "    \"val_loss\": history[\"epoch/val_loss\"].tolist(),\n",
        "    \"f1_score\": history[\"epoch/f1_score\"].tolist(),\n",
        "    \"val_f1_score\": history[\"epoch/val_f1_score\"].tolist(),\n",
        "    \"accuracy\": history[\"epoch/accuracy\"].tolist(),\n",
        "    \"val_accuracy\": history[\"epoch/val_accuracy\"].tolist(),\n",
        "    \"precision\": history[\"epoch/precision\"].tolist(),\n",
        "    \"val_precision\": history[\"epoch/val_precision\"].tolist(),\n",
        "    \"recall\": history[\"epoch/recall\"].tolist(),\n",
        "    \"val_recall\": history[\"epoch/val_recall\"].tolist(),\n",
        "    \"auc\": history[\"epoch/auc\"].tolist(),\n",
        "    \"val_auc\": history[\"epoch/val_auc\"].tolist(),\n",
        "    \"learning_rate\": history[\"epoch/learning_rate\"].tolist(),  # No val version\n",
        "}\n",
        "\n",
        "# Define function to re-plot and log (handling missing val_ cases)\n",
        "def plot_and_log_metric(metric_name, title, has_val=True):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(history_dict[metric_name], label=\"Train \" + metric_name.capitalize())\n",
        "\n",
        "    # Only plot validation if it exists\n",
        "    if has_val:\n",
        "        plt.plot(history_dict[\"val_\" + metric_name], label=\"Validation \" + metric_name.capitalize())\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric_name.capitalize())\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.savefig(f\"{metric_name}_plot.png\")\n",
        "    mlflow.log_artifact(f\"{metric_name}_plot.png\")  # Log to MLflow\n",
        "    wandb.log({f\"{metric_name}_plot\": wandb.Image(f\"{metric_name}_plot.png\")})  # Log to W&B\n",
        "    plt.close()\n",
        "\n",
        "# Recreate and log all plots\n",
        "plot_and_log_metric(\"loss\", \"Loss over Epochs\")\n",
        "plot_and_log_metric(\"f1_score\", \"F1 Score over Epochs\")\n",
        "plot_and_log_metric(\"accuracy\", \"Accuracy over Epochs\")\n",
        "plot_and_log_metric(\"precision\", \"Precision over Epochs\")\n",
        "plot_and_log_metric(\"recall\", \"Recall over Epochs\")\n",
        "plot_and_log_metric(\"auc\", \"AUC over Epochs\")\n",
        "plot_and_log_metric(\"learning_rate\", \"Learning Rate over Epochs\", has_val=False)  # No validation version\n",
        "\n",
        "print(\"Plots regenerated and logged successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knU8GVkrP-uf",
        "outputId": "23385a00-6beb-4146-84c8-cc2e4dbedeb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025/03/06 02:20:07 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
            "\u001b[31m2025/03/06 02:20:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and metrics have been logged successfully.\n",
            "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
            "       'end_time', 'metrics.loss', 'metrics.auc', 'metrics.val_auc',\n",
            "       'metrics.val_recall', 'metrics.accuracy', 'metrics.f1_score',\n",
            "       'metrics.val_accuracy', 'metrics.val_loss', 'metrics.precision',\n",
            "       'metrics.val_f1_score', 'metrics.val_precision', 'metrics.recall',\n",
            "       'tags.mlflow.source.name', 'tags.mlflow.source.type',\n",
            "       'tags.mlflow.runName', 'tags.mlflow.log-model.history',\n",
            "       'tags.mlflow.user'],\n",
            "      dtype='object')\n",
            "                             run_id  metrics.accuracy  metrics.loss\n",
            "0  fea7e2f8895642a793bf38a8a7362382               NaN           NaN\n",
            "1  8942431403c44dada182b7fbf4fc3f40               NaN           NaN\n",
            "2  0f1e7ada560a4f649a7ceea3e754f337               NaN           NaN\n",
            "3  f3426f57943d41068963920ab123d900               NaN           NaN\n",
            "4  a49a4770dcc14ec8b5f24786add8d9f0               NaN           NaN\n",
            "5  86ac75d6a3f44eff836c0aee3efe2804               NaN           NaN\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from tensorflow.keras.models import load_model  # Correct way to load a Keras model\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model(\"/content/toxicity_model.h5\")\n",
        "\n",
        "# End any active MLflow run if it exists\n",
        "mlflow.end_run()\n",
        "\n",
        "# If experiment does not exist, create one\n",
        "experiment_name = \"toxicity_detection\"\n",
        "try:\n",
        "    experiment = mlflow.create_experiment(experiment_name)\n",
        "except:\n",
        "    # If experiment already exists, just get it\n",
        "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "\n",
        "# Start a new run within the experiment\n",
        "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
        "    # Log the model\n",
        "    mlflow.keras.log_model(model, \"model\")\n",
        "\n",
        "    # Assuming you have history_dict available (from your training history)\n",
        "    # Log metrics to MLflow (no retraining required)\n",
        "    for epoch in range(len(history_dict[\"loss\"])):\n",
        "        mlflow.log_metric(\"loss\", history_dict[\"loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", history_dict[\"val_loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"accuracy\", history_dict[\"accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", history_dict[\"val_accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"f1_score\", history_dict[\"f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_f1_score\", history_dict[\"val_f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"precision\", history_dict[\"precision\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_precision\", history_dict[\"val_precision\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"recall\", history_dict[\"recall\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_recall\", history_dict[\"val_recall\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"auc\", history_dict[\"auc\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_auc\", history_dict[\"val_auc\"][epoch], step=epoch)\n",
        "\n",
        "    # Log the plots that were saved during training\n",
        "    for metric_name in [\"loss\", \"f1_score\", \"accuracy\", \"precision\", \"recall\", \"auc\"]:\n",
        "        mlflow.log_artifact(f\"{metric_name}_plot.png\")\n",
        "\n",
        "print(\"Model and metrics have been logged successfully.\")\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "\n",
        "# Get experiment by name (or ID)\n",
        "experiment = mlflow.get_experiment_by_name(\"toxicity_detection\")\n",
        "\n",
        "# Retrieve all runs\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "print(runs.columns)\n",
        "\n",
        "# Print the metrics for each run (e.g., accuracy, loss)\n",
        "print(runs[[\"run_id\", \"metrics.accuracy\", \"metrics.loss\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgWxs-UKNLoe",
        "outputId": "14ba24a3-9722-4990-8a89-65083c8e9475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
            "       'end_time', 'metrics.loss', 'metrics.auc', 'metrics.val_auc',\n",
            "       'metrics.val_recall', 'metrics.accuracy', 'metrics.f1_score',\n",
            "       'metrics.val_accuracy', 'metrics.val_loss', 'metrics.precision',\n",
            "       'metrics.val_f1_score', 'metrics.val_precision', 'metrics.recall',\n",
            "       'tags.mlflow.source.name', 'tags.mlflow.source.type',\n",
            "       'tags.mlflow.runName', 'tags.mlflow.log-model.history',\n",
            "       'tags.mlflow.user'],\n",
            "      dtype='object')\n",
            "                             run_id  metrics.accuracy  metrics.loss\n",
            "0  a49a4770dcc14ec8b5f24786add8d9f0               NaN           NaN\n",
            "1  86ac75d6a3f44eff836c0aee3efe2804               NaN           NaN\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.keras\n",
        "\n",
        "# Get experiment by name (or ID)\n",
        "experiment = mlflow.get_experiment_by_name(\"toxicity_detection\")\n",
        "\n",
        "# Retrieve all runs\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "print(runs.columns)\n",
        "\n",
        "# Print the metrics for each run (e.g., accuracy, loss)\n",
        "print(runs[[\"run_id\", \"metrics.accuracy\", \"metrics.loss\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8P2ege9QBlH"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "!mlflow ui --port 5000 &\n",
        "public_url = ngrok.connect(port=\"5000\")\n",
        "print(f\"MLflow Tracking UI: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERYj6qunyPn7",
        "outputId": "8a9b5555-55d3-44e1-a5d4-39eb46ae198a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "\n",
        "custom_objects = {\"LSTM\": LSTM, \"Bidirectional\": Bidirectional}\n",
        "\n",
        "model = tf.keras.models.load_model('/content/toxicity_model.h5', custom_objects=custom_objects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn5XHZnnClgT"
      },
      "source": [
        "Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEnDqNuwxYE-",
        "outputId": "d9c3a599-7ab7-4dfc-9392-34d5d835a498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
            "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
            "       'identity_hate'],\n",
            "      dtype='object')\n",
            "[[0.6889598  0.00483085 0.07420572 0.0524078  0.14637259 0.10724357]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
            "       'identity_hate'],\n",
            "      dtype='object')\n",
            "[[5.3074697e-11 0.0000000e+00 2.5806218e-19 8.3492614e-38 8.7714351e-24\n",
            "  1.4263595e-30]]\n"
          ]
        }
      ],
      "source": [
        "input_str = vectorizer(\"Heyy I hate you\")\n",
        "res = model.predict(np.expand_dims(input_str, axis = 0))\n",
        "# printout the columns\n",
        "print(df.columns[2:])\n",
        "print(res)\n",
        "\n",
        "input_str = vectorizer(\"You looks very nice\")\n",
        "res = model.predict(np.expand_dims(input_str, axis = 0))\n",
        "# printout the columns\n",
        "print(df.columns[2:])\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq-HdMxQHnhp"
      },
      "source": [
        "Evaluate Model on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahinTbSDmUGC",
        "outputId": "b314fa69-48af-4ee3-e5dd-7539570adabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1],\n",
              "       [0, 1, 0, 1, 1, 1]])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_X, batch_y = test.as_numpy_iterator().next()\n",
        "(model.predict(batch_X) > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roLBHIAHmaPT",
        "outputId": "a6050346-019d-465a-d043-91d21e118af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 6)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WUAUnbsHmTm",
        "outputId": "9602b103-3f62-45b6-e68d-f8fed3949756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m997/997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 178ms/step - accuracy: 0.9877 - auc: 0.9975 - f1_score: 0.2112 - loss: 0.0091 - precision: 0.9523 - recall: 0.9477\n",
            "Test Accuracy: 0.9866\n",
            "Test F1-Score: 0.9498\n",
            "Test recall: 0.9498\n",
            "Test Precision: 0.9498\n",
            "Test loss: 0.0095\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Model on Test Data\n",
        "test_results = model.evaluate(test, verbose=1)\n",
        "\n",
        "# Extract Metrics\n",
        "test_loss = test_results[0]\n",
        "test_accuracy = test_results[1]\n",
        "test_precision = test_results[2]\n",
        "test_recall = test_results[3]\n",
        "test_auc = test_results[4]\n",
        "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall + tf.keras.backend.epsilon())\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Test recall: {test_recall:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "dCIjMWsxJtNj",
        "outputId": "2f2c4258-e740-43ae-b509-cc30d61f0ee4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_auc</td><td>▁</td></tr><tr><td>test_f1_score</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.98665</td></tr><tr><td>test_auc</td><td>0.99822</td></tr><tr><td>test_f1_score</td><td>0.94977</td></tr><tr><td>test_loss</td><td>0.00947</td></tr><tr><td>test_precision</td><td>0.94977</td></tr><tr><td>test_recall</td><td>0.94977</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">different-brook-3</strong> at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/vr3nj828' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/vr3nj828</a><br> View project at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a><br>Synced 5 W&B file(s), 27 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250306_002806-vr3nj828/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ensure W&B is initialized before logging\n",
        "wandb.init(project=\"toxicity-detection\", name=\"test_evaluation\")\n",
        "\n",
        "# Log Test Metrics to W&B\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy,\n",
        "    \"test_precision\": test_precision,\n",
        "    \"test_recall\": test_recall,\n",
        "    \"test_f1_score\": test_f1,\n",
        "    \"test_auc\": test_auc\n",
        "})\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrs_MdtcJtU0",
        "outputId": "ebf5ae18-3c94-45b5-be20-f644d090641f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.11 pyngrok-7.2.3 starlette-0.46.0 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRSCH5bcJ7zp",
        "outputId": "51936e38-e8b2-4a86-d774-6a0698080437"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "app =FastAPI()\n",
        "\n",
        "model =tf.keras.models.load_model(\"/content/toxicity_model.h5\")\n",
        "class InputText(BaseModel):\n",
        "  text:str\n",
        "@app.post(\"/predict\")\n",
        "async def predict(input_text : InputText):\n",
        "  text: input_text.text\n",
        "  vect_text = vectorizer(text)\n",
        "  prediction = model.predict([text])\n",
        "  return {\"prediction\": prediction.tolist()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo8LYgHHPydc"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken 2tVdIx30fQWA14bNtDn4lfBR9tE_77bGYGjjdXZRXD17uKHy5\n",
        "!mlflow ui --port 5000 &\n",
        "public_url = ngrok.connect(port=\"5000\")\n",
        "print(f\"MLflow Tracking UI: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-PeZpZpM06m",
        "outputId": "7f7b0585-9bfc-4a01-eb3b-a0a16af9e7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "FastAPI app is accessible at: NgrokTunnel: \"https://5b11-34-59-186-82.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m11838\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"app\".\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken 2tVdIx30fQWA14bNtDn4lfBR9tE_77bGYGjjdXZRXD17uKHy5\n",
        "\n",
        "!uvicorn --host 0.0.0.0 --port 8000 --reload app:app\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"FastAPI app is accessible at:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3oOVU-eJ2fq",
        "outputId": "05fc9afd-49ae-43c5-edcc-e98334fe4f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Test results logged in MLflow\n"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run():\n",
        "    # Log Test Metrics\n",
        "    mlflow.log_metric(\"test_loss\", test_loss)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_precision\", test_precision)\n",
        "    mlflow.log_metric(\"test_recall\", test_recall)\n",
        "    mlflow.log_metric(\"test_auc\", test_auc)\n",
        "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
        "\n",
        "    print(\" Test results logged in MLflow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxLDEbolsUT"
      },
      "source": [
        "Test using Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpsCwP3oLxNR",
        "outputId": "ccd4b9c8-4068-44eb-cded-59cd91b58388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.17.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.1 (from gradio)\n",
            "  Downloading gradio_client-1.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.17.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.1-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.17.1 gradio-client-1.7.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, WebSocket\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "#  Load the trained model\n",
        "model = tf.keras.models.load_model(\"toxicity_model.h5\")\n",
        "\n",
        "#  Recreate the `TextVectorization` layer\n",
        "MAX_WORDS = 200000\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_WORDS, output_sequence_length=1800, output_mode='int')\n",
        "\n",
        "#  Define Toxicity Labels\n",
        "TOXICITY_LABELS = [\"Toxic\", \"Severe Toxic\", \"Obscene\", \"Threat\", \"Insult\", \"Identity Hate\"]\n",
        "\n",
        "#  Load the saved vocabulary\n",
        "if os.path.exists(\"vectorizer_vocab.pkl\"):\n",
        "    with open(\"vectorizer_vocab.pkl\", \"rb\") as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    if vocab:\n",
        "        vectorizer.set_vocabulary(vocab)\n",
        "        print(\" Vocabulary loaded successfully!\")\n",
        "    else:\n",
        "        print(\"🚨 Error: Loaded vocabulary is empty!\")\n",
        "        exit(1)\n",
        "else:\n",
        "    print(\"🚨 Error: vectorizer_vocab.pkl not found!\")\n",
        "    exit(1)\n",
        "\n",
        "#  Ensure Model is Compiled\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#  Store connected users\n",
        "connected_users = set()\n",
        "\n",
        "#  Function to Predict Toxicity & Return Scores\n",
        "def predict_toxicity(message):\n",
        "    if not message.strip():\n",
        "        return [0.0] * 6\n",
        "\n",
        "    input_text = tf.expand_dims(message, axis=0)\n",
        "    input_text = vectorizer(input_text)\n",
        "\n",
        "    prediction = model.predict(input_text)[0]\n",
        "    toxicity_scores = {label : round(float(score),4) for label,score in zip(TOXICITY_LABELS, prediction)}\n",
        "    return toxicity_scores  # Returns list of scores\n",
        "\n",
        "#  Start FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.websocket(\"/chat\")\n",
        "async def chatroom(websocket: WebSocket):\n",
        "    await websocket.accept()\n",
        "    connected_users.add(websocket)\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            #  Receive message from user\n",
        "            data = await websocket.receive_text()\n",
        "            message_data = json.loads(data)\n",
        "            message = message_data[\"message\"]\n",
        "            user = message_data[\"user\"]\n",
        "\n",
        "            #  Predict toxicity & get max score\n",
        "            toxicity_scores = predict_toxicity(message)\n",
        "            max_toxicity = max(toxicity_scores.values())\n",
        "\n",
        "            # display color according to toxic level\n",
        "            def format_toxicity_scores(toxicity_scores):\n",
        "                return \"\\n\".join(\n",
        "                    f\"{'🔴' if score >= 0.7 else '🟢'} {category}: {score * 100:.2f}%\"\n",
        "                    for category, score in toxicity_scores.items()\n",
        "                )\n",
        "\n",
        "            if max_toxicity > 0.7:\n",
        "                #  Send warning **only to the sender** with full toxicity details\n",
        "                formatted_scores = format_toxicity_scores(toxicity_scores)\n",
        "                await websocket.send_json({\n",
        "                    \"user\": \"assistant\",\n",
        "                    \"message\": \"🚨 Message Blocked Due to High Toxicity!\" + formatted_scores\n",
        "                })\n",
        "                continue  # Don't broadcast toxic messages\n",
        "\n",
        "            #  Broadcast Clean Messages + Toxicity Score to Everyone\n",
        "            response_message = {\n",
        "                \"user\": user,\n",
        "                \"message\": message,\n",
        "                \"toxicity_score\": max_toxicity  # Include toxicity score\n",
        "            }\n",
        "            for user_socket in connected_users:\n",
        "                await user_socket.send_json(response_message)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    finally:\n",
        "        connected_users.remove(websocket)  # Remove disconnected users\n",
        "\n",
        "#  Run FastAPI Server Locally\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cBmqJm5a6_l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8plwfwX5XJu-",
        "outputId": "be78a0bf-db93-4fa7-8b0c-ed3e9d1141e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ],
      "source": [
        "pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEOm84m-XmNJ",
        "outputId": "f0136f25-9f73-41df-c33d-398ca0f55d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.17.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (14.2)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (2024.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok gradio tensorflow websockets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UZFZ38dZJs7",
        "outputId": "5812b52b-312b-4b6b-8764-8988e38bc1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken XUV4QONYGOEHMJ26ZWV23SI6E53S3FXK"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}