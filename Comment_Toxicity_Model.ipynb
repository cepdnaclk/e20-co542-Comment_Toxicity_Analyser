{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow\n"
      ],
      "metadata": {
        "id": "LcSkRJz5ykpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e52d7a-b933-4a43-df8b-5b92914562c3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.20.3)\n",
            "Requirement already satisfied: mlflow-skinny==2.20.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.20.3)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.44.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.25.6)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.10.6)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.12.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (75.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.37b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2025.1.31)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import os # navigate through defferent files\n",
        "import numpy as np\n",
        "import wandb\n",
        "import mlflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.backend import epsilon"
      ],
      "metadata": {
        "id": "vc-atc5ift0t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "fh7f2sKOtsv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import mlflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize Weights & Biases (W&B)\n",
        "wandb.login()\n",
        "wandb.init(project=\"toxicity-detection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "snVGaXC3yVuU",
        "outputId": "85220a2b-4e9c-490a-b0fe-d6828b7de854"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me20189\u001b[0m (\u001b[33me20189-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250306_081102-s1h33ug6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/s1h33ug6' target=\"_blank\">charmed-tree-5</a></strong> to <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/s1h33ug6' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/s1h33ug6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/s1h33ug6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b0d1e25e1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/CommentToxicity/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv') # train data set"
      ],
      "metadata": {
        "id": "2UsNoiYSjy4d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['toxic'] ==1].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "knZGZdeqkvOl",
        "outputId": "2bdfc118-0734-4e36-f987-b4b508c09189"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                       comment_text  \\\n",
              "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
              "12  0005c987bdfc9d4b  Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...   \n",
              "16  0007e25b2121310b  Bye! \\r\\n\\r\\nDon't look, come or think of comm...   \n",
              "42  001810bf8c45bf5f  You are gay or antisemmitian? \\r\\n\\r\\nArchange...   \n",
              "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
              "\n",
              "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
              "6       1             1        1       0       1              0  \n",
              "12      1             0        0       0       0              0  \n",
              "16      1             0        0       0       0              0  \n",
              "42      1             0        1       0       1              1  \n",
              "43      1             0        1       0       1              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ef073a0-8e32-4a35-a3f9-977ce53a1581\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0005c987bdfc9d4b</td>\n",
              "      <td>Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0007e25b2121310b</td>\n",
              "      <td>Bye! \\r\\n\\r\\nDon't look, come or think of comm...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>001810bf8c45bf5f</td>\n",
              "      <td>You are gay or antisemmitian? \\r\\n\\r\\nArchange...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>00190820581d90ce</td>\n",
              "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ef073a0-8e32-4a35-a3f9-977ce53a1581')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5ef073a0-8e32-4a35-a3f9-977ce53a1581 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5ef073a0-8e32-4a35-a3f9-977ce53a1581');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-de7cc500-1ef5-4fa2-970c-803b3482b5e7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de7cc500-1ef5-4fa2-970c-803b3482b5e7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-de7cc500-1ef5-4fa2-970c-803b3482b5e7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[df['toxic'] ==1]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0005c987bdfc9d4b\",\n          \"00190820581d90ce\",\n          \"0007e25b2121310b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\r\\n\\r\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...\",\n          \"FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\",\n          \"Bye! \\r\\n\\r\\nDon't look, come or think of comming back! Tosser.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"severe_toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"obscene\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insult\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity_hate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id = 12\n",
        "df.iloc[id]['comment_text']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "TbZOxomWpt4e",
        "outputId": "f177bc11-33bd-470c-a646-5c17c0dcce89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\r\\n\\r\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.columns[2:]].iloc[id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "E-RnzBmfpvat",
        "outputId": "73fc1aa6-3d3f-47a2-e592-9c8efa40ea4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic            1\n",
              "severe_toxic     0\n",
              "obscene          0\n",
              "threat           0\n",
              "insult           0\n",
              "identity_hate    0\n",
              "Name: 12, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>toxic</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>severe_toxic</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>obscene</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>threat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insult</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>identity_hate</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "aG8k2IdDtvfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "wq_2ORSstxqG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeN5A-x8uIEe",
        "outputId": "68e99ad1-1f02-4c42-971c-141351274443"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
              "       'insult', 'identity_hate'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[df.columns[2:]].values # converting the measuring values in the data frames to a matrix as labels"
      ],
      "metadata": {
        "id": "sJgkGkrbuvMd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df['comment_text'] # take the comment text as inputs"
      ],
      "metadata": {
        "id": "kKwoL0VKvaPe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 200000 # Number of vocab in the dictionary\n",
        "vectorizer = TextVectorization(max_tokens=MAX_WORDS , output_sequence_length=1800 ,output_mode='int') # cap the max length of inputs to 1800 words and the model outputs data in the integer format"
      ],
      "metadata": {
        "id": "JeFwRH0-wahN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.adapt(x.values) # fit the vectorizer to the data , learn the words in the inputs"
      ],
      "metadata": {
        "id": "hR_G6Np5xus2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Save vectorizer weights\n",
        "vectorizer_weights = vectorizer.get_weights()\n",
        "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer_weights, f)\n"
      ],
      "metadata": {
        "id": "PmVFRQBwfTDJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "#  Save the vocabulary\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "with open(\"vectorizer_vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "print(\" Vocabulary saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9h736xFhRzl",
        "outputId": "f9419789-8f15-463f-b191-d20f522ca467"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSn7rwEty56O",
        "outputId": "ae121084-4bbc-47d2-918a-9b186f458bd3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'to',\n",
              " 'of',\n",
              " 'and',\n",
              " 'a',\n",
              " 'you',\n",
              " 'i',\n",
              " 'is',\n",
              " 'that',\n",
              " 'in',\n",
              " 'it',\n",
              " 'for',\n",
              " 'this',\n",
              " 'not',\n",
              " 'on',\n",
              " 'be',\n",
              " 'as',\n",
              " 'have',\n",
              " 'are',\n",
              " 'your',\n",
              " 'with',\n",
              " 'if',\n",
              " 'article',\n",
              " 'was',\n",
              " 'or',\n",
              " 'but',\n",
              " 'page',\n",
              " 'my',\n",
              " 'an',\n",
              " 'from',\n",
              " 'by',\n",
              " 'do',\n",
              " 'at',\n",
              " 'about',\n",
              " 'me',\n",
              " 'so',\n",
              " 'wikipedia',\n",
              " 'can',\n",
              " 'what',\n",
              " 'there',\n",
              " 'all',\n",
              " 'has',\n",
              " 'will',\n",
              " 'talk',\n",
              " 'please',\n",
              " 'would',\n",
              " 'its',\n",
              " 'no',\n",
              " 'one',\n",
              " 'just',\n",
              " 'like',\n",
              " 'they',\n",
              " 'he',\n",
              " 'dont',\n",
              " 'which',\n",
              " 'any',\n",
              " 'been',\n",
              " 'should',\n",
              " 'more',\n",
              " 'we',\n",
              " 'some',\n",
              " 'other',\n",
              " 'who',\n",
              " 'see',\n",
              " 'here',\n",
              " 'also',\n",
              " 'his',\n",
              " 'think',\n",
              " 'im',\n",
              " 'because',\n",
              " 'know',\n",
              " 'how',\n",
              " 'am',\n",
              " 'people',\n",
              " 'why',\n",
              " 'edit',\n",
              " 'articles',\n",
              " 'only',\n",
              " 'out',\n",
              " 'up',\n",
              " 'when',\n",
              " 'were',\n",
              " 'use',\n",
              " 'then',\n",
              " 'may',\n",
              " 'time',\n",
              " 'did',\n",
              " 'them',\n",
              " 'now',\n",
              " 'being',\n",
              " 'their',\n",
              " 'than',\n",
              " 'thanks',\n",
              " 'even',\n",
              " 'get',\n",
              " 'make',\n",
              " 'good',\n",
              " 'had',\n",
              " 'very',\n",
              " 'information',\n",
              " 'does',\n",
              " 'could',\n",
              " 'well',\n",
              " 'want',\n",
              " 'such',\n",
              " 'sources',\n",
              " 'way',\n",
              " 'name',\n",
              " 'these',\n",
              " 'deletion',\n",
              " 'pages',\n",
              " 'first',\n",
              " 'help',\n",
              " 'new',\n",
              " 'editing',\n",
              " 'source',\n",
              " 'go',\n",
              " 'need',\n",
              " 'say',\n",
              " 'section',\n",
              " 'edits',\n",
              " 'again',\n",
              " 'thank',\n",
              " 'where',\n",
              " 'user',\n",
              " 'made',\n",
              " 'many',\n",
              " 'much',\n",
              " 'really',\n",
              " 'used',\n",
              " 'most',\n",
              " 'discussion',\n",
              " 'find',\n",
              " 'same',\n",
              " 'ive',\n",
              " 'deleted',\n",
              " 'into',\n",
              " 'fuck',\n",
              " 'those',\n",
              " 'work',\n",
              " 'since',\n",
              " 'before',\n",
              " 'after',\n",
              " 'point',\n",
              " 'add',\n",
              " 'look',\n",
              " 'right',\n",
              " 'read',\n",
              " 'image',\n",
              " 'take',\n",
              " 'still',\n",
              " 'over',\n",
              " 'someone',\n",
              " 'him',\n",
              " 'two',\n",
              " 'back',\n",
              " 'too',\n",
              " 'fact',\n",
              " 'link',\n",
              " 'said',\n",
              " 'own',\n",
              " 'something',\n",
              " 'going',\n",
              " 'youre',\n",
              " 'blocked',\n",
              " 'list',\n",
              " 'stop',\n",
              " 'without',\n",
              " 'content',\n",
              " 'hi',\n",
              " 'under',\n",
              " 'editors',\n",
              " 'our',\n",
              " 'block',\n",
              " 'thats',\n",
              " 'us',\n",
              " 'added',\n",
              " 'utc',\n",
              " 'history',\n",
              " 'another',\n",
              " 'doesnt',\n",
              " 'removed',\n",
              " 'might',\n",
              " 'note',\n",
              " 'however',\n",
              " 'sure',\n",
              " 'place',\n",
              " 'never',\n",
              " 'done',\n",
              " 'welcome',\n",
              " 'her',\n",
              " 'case',\n",
              " 'put',\n",
              " 'personal',\n",
              " 'seems',\n",
              " 'reason',\n",
              " 'better',\n",
              " 'using',\n",
              " 'yourself',\n",
              " 'cant',\n",
              " 'actually',\n",
              " 'ask',\n",
              " 'comment',\n",
              " 'while',\n",
              " 'vandalism',\n",
              " 'feel',\n",
              " 'question',\n",
              " 'anything',\n",
              " 'believe',\n",
              " 'person',\n",
              " 'links',\n",
              " 'things',\n",
              " 'both',\n",
              " 'didnt',\n",
              " 'comments',\n",
              " 'best',\n",
              " 'ill',\n",
              " 'part',\n",
              " 'she',\n",
              " 'hope',\n",
              " 'policy',\n",
              " 'against',\n",
              " 'off',\n",
              " 'keep',\n",
              " 'already',\n",
              " 'free',\n",
              " 'wiki',\n",
              " 'thing',\n",
              " 'nothing',\n",
              " 'change',\n",
              " 'wrong',\n",
              " 'though',\n",
              " 'problem',\n",
              " 'remove',\n",
              " 'little',\n",
              " 'subject',\n",
              " '',\n",
              " 'others',\n",
              " 'trying',\n",
              " 'tag',\n",
              " 'copyright',\n",
              " 'must',\n",
              " 'understand',\n",
              " 'above',\n",
              " 'few',\n",
              " 'anyone',\n",
              " 'speedy',\n",
              " 'last',\n",
              " 'issue',\n",
              " 'give',\n",
              " 'questions',\n",
              " 'agree',\n",
              " 'rather',\n",
              " 'years',\n",
              " 'let',\n",
              " '2',\n",
              " 'different',\n",
              " 'editor',\n",
              " 'long',\n",
              " 'reliable',\n",
              " 'making',\n",
              " 'world',\n",
              " 'come',\n",
              " 'sorry',\n",
              " 'isnt',\n",
              " 'reference',\n",
              " 'mean',\n",
              " 'continue',\n",
              " 'try',\n",
              " 'references',\n",
              " 'found',\n",
              " 'doing',\n",
              " 'text',\n",
              " 'great',\n",
              " 'leave',\n",
              " 'says',\n",
              " 'got',\n",
              " 'probably',\n",
              " 'english',\n",
              " 'original',\n",
              " 'every',\n",
              " '1',\n",
              " 'simply',\n",
              " 'word',\n",
              " 'users',\n",
              " 'fair',\n",
              " 'hello',\n",
              " 'either',\n",
              " 'check',\n",
              " 'least',\n",
              " 'adding',\n",
              " 'ip',\n",
              " 'show',\n",
              " 'site',\n",
              " 'state',\n",
              " 'else',\n",
              " 'delete',\n",
              " 'consensus',\n",
              " 'enough',\n",
              " 'request',\n",
              " 'far',\n",
              " 'opinion',\n",
              " 'created',\n",
              " 'around',\n",
              " 'life',\n",
              " 'day',\n",
              " 'between',\n",
              " 'through',\n",
              " 'example',\n",
              " 'view',\n",
              " 'yes',\n",
              " 'reverted',\n",
              " 'yet',\n",
              " 'etc',\n",
              " 'id',\n",
              " 'matter',\n",
              " 'shit',\n",
              " 'u',\n",
              " 'war',\n",
              " 'notable',\n",
              " 'contributions',\n",
              " 'given',\n",
              " 'thought',\n",
              " 'material',\n",
              " 'book',\n",
              " 'admin',\n",
              " 'write',\n",
              " 'post',\n",
              " 'down',\n",
              " 'account',\n",
              " 'clearly',\n",
              " 'having',\n",
              " 'encyclopedia',\n",
              " 'lot',\n",
              " 'support',\n",
              " 'real',\n",
              " 'bad',\n",
              " 'message',\n",
              " 'needs',\n",
              " 'images',\n",
              " 'tell',\n",
              " 'seem',\n",
              " 'called',\n",
              " 'maybe',\n",
              " 'evidence',\n",
              " 'instead',\n",
              " 'ever',\n",
              " '3',\n",
              " 'correct',\n",
              " 'saying',\n",
              " 'clear',\n",
              " 'always',\n",
              " 'number',\n",
              " 'important',\n",
              " 'further',\n",
              " 'quite',\n",
              " 'perhaps',\n",
              " 'old',\n",
              " '',\n",
              " 'true',\n",
              " 'until',\n",
              " 'hate',\n",
              " 'states',\n",
              " 'whether',\n",
              " 'consider',\n",
              " 'written',\n",
              " 'claim',\n",
              " 'language',\n",
              " 'media',\n",
              " 'bit',\n",
              " 'once',\n",
              " 'guidelines',\n",
              " 'term',\n",
              " 'criteria',\n",
              " 'research',\n",
              " 'nigger',\n",
              " 'version',\n",
              " 'times',\n",
              " 'website',\n",
              " 'getting',\n",
              " 'fucking',\n",
              " 'theres',\n",
              " 'review',\n",
              " 'mention',\n",
              " 'pov',\n",
              " 'oh',\n",
              " 'makes',\n",
              " 'several',\n",
              " 'revert',\n",
              " 'considered',\n",
              " 'changes',\n",
              " 'cannot',\n",
              " 'words',\n",
              " 'idea',\n",
              " 'title',\n",
              " 'suck',\n",
              " 'address',\n",
              " 'notice',\n",
              " 'based',\n",
              " 'top',\n",
              " 'following',\n",
              " 'current',\n",
              " 'each',\n",
              " 'listed',\n",
              " 'means',\n",
              " 'possible',\n",
              " 'group',\n",
              " 'facts',\n",
              " 'regarding',\n",
              " 'care',\n",
              " 'rules',\n",
              " 'second',\n",
              " 'main',\n",
              " 'template',\n",
              " 'mentioned',\n",
              " 'general',\n",
              " 'year',\n",
              " 'attack',\n",
              " 'kind',\n",
              " 'whole',\n",
              " 'course',\n",
              " 'statement',\n",
              " 'left',\n",
              " 'hey',\n",
              " 'date',\n",
              " 'include',\n",
              " 'seen',\n",
              " 'three',\n",
              " 'issues',\n",
              " 'start',\n",
              " 'ass',\n",
              " 'ok',\n",
              " 'end',\n",
              " 'wikipedias',\n",
              " 'call',\n",
              " 'less',\n",
              " 'topic',\n",
              " 'gay',\n",
              " 'suggest',\n",
              " 'man',\n",
              " 'including',\n",
              " 'happy',\n",
              " 'sense',\n",
              " 'provide',\n",
              " 'create',\n",
              " 'big',\n",
              " 'days',\n",
              " 'myself',\n",
              " 'american',\n",
              " 'redirect',\n",
              " 'known',\n",
              " 'sentence',\n",
              " 'move',\n",
              " 'appropriate',\n",
              " 'changed',\n",
              " 'love',\n",
              " 'notability',\n",
              " 'explain',\n",
              " 'started',\n",
              " 'included',\n",
              " 'removing',\n",
              " 'project',\n",
              " 'anyway',\n",
              " 'info',\n",
              " 'mind',\n",
              " 'school',\n",
              " '2005',\n",
              " 'next',\n",
              " 'looking',\n",
              " 'although',\n",
              " 'picture',\n",
              " 'relevant',\n",
              " 'four',\n",
              " 'die',\n",
              " 'sign',\n",
              " 'answer',\n",
              " 'style',\n",
              " 'away',\n",
              " 'per',\n",
              " 'order',\n",
              " 'warning',\n",
              " 'wont',\n",
              " 'recent',\n",
              " 'youve',\n",
              " 'interest',\n",
              " 'community',\n",
              " 'summary',\n",
              " 'later',\n",
              " 'lol',\n",
              " 'claims',\n",
              " 'currently',\n",
              " 'discuss',\n",
              " 'interested',\n",
              " 'policies',\n",
              " 'attacks',\n",
              " 'especially',\n",
              " 'wish',\n",
              " 'wrote',\n",
              " 'able',\n",
              " 'specific',\n",
              " 'public',\n",
              " 'taken',\n",
              " 'writing',\n",
              " 'neutral',\n",
              " 'full',\n",
              " 'names',\n",
              " 'within',\n",
              " '4',\n",
              " 'position',\n",
              " 'related',\n",
              " 'below',\n",
              " 'line',\n",
              " 'wanted',\n",
              " 'during',\n",
              " 'appears',\n",
              " 'stuff',\n",
              " 'certainly',\n",
              " 'official',\n",
              " 'nice',\n",
              " 'itself',\n",
              " 'faith',\n",
              " 'everyone',\n",
              " 'wasnt',\n",
              " 'live',\n",
              " 'report',\n",
              " 'completely',\n",
              " 'according',\n",
              " 'unless',\n",
              " 'common',\n",
              " 'pretty',\n",
              " 'country',\n",
              " 'everything',\n",
              " 'looks',\n",
              " 'due',\n",
              " 'single',\n",
              " 'hes',\n",
              " 'process',\n",
              " 'contribs',\n",
              " 'news',\n",
              " 'involved',\n",
              " 'god',\n",
              " 'fat',\n",
              " 'therefore',\n",
              " 'obviously',\n",
              " 'remember',\n",
              " 'lead',\n",
              " 'hard',\n",
              " 'admins',\n",
              " 'came',\n",
              " 'edited',\n",
              " 'web',\n",
              " 'stay',\n",
              " 'learn',\n",
              " 'response',\n",
              " 'future',\n",
              " 'past',\n",
              " 'asked',\n",
              " 'truth',\n",
              " 'reading',\n",
              " 'power',\n",
              " '2006',\n",
              " 'stupid',\n",
              " 'entry',\n",
              " 'quote',\n",
              " 'posted',\n",
              " 'nor',\n",
              " 'talking',\n",
              " 'placed',\n",
              " '5',\n",
              " 'ago',\n",
              " 'similar',\n",
              " 'email',\n",
              " 'game',\n",
              " 'published',\n",
              " 'exactly',\n",
              " 'today',\n",
              " 'reasons',\n",
              " 'paragraph',\n",
              " 'faggot',\n",
              " 'city',\n",
              " 'argument',\n",
              " 'whatever',\n",
              " 'system',\n",
              " 'working',\n",
              " 'false',\n",
              " 'sandbox',\n",
              " 'moron',\n",
              " 'political',\n",
              " 'noticed',\n",
              " 'useful',\n",
              " 'havent',\n",
              " 'guy',\n",
              " 'high',\n",
              " 'regards',\n",
              " 'united',\n",
              " 'guess',\n",
              " 'appreciate',\n",
              " 'particular',\n",
              " 'deleting',\n",
              " 'form',\n",
              " 'books',\n",
              " 'government',\n",
              " 'dispute',\n",
              " 'five',\n",
              " 'british',\n",
              " 'reverting',\n",
              " 'major',\n",
              " 'problems',\n",
              " 'national',\n",
              " 'party',\n",
              " 'provided',\n",
              " 'often',\n",
              " 'ones',\n",
              " 'become',\n",
              " 'lets',\n",
              " 'tried',\n",
              " 'side',\n",
              " 'administrator',\n",
              " 'along',\n",
              " 'reply',\n",
              " 'almost',\n",
              " 'needed',\n",
              " 'stated',\n",
              " 'rule',\n",
              " 'took',\n",
              " 'search',\n",
              " 'knowledge',\n",
              " 'banned',\n",
              " 'cheers',\n",
              " 'taking',\n",
              " 'vandalize',\n",
              " '',\n",
              " 'certain',\n",
              " '2007',\n",
              " 'username',\n",
              " 'fine',\n",
              " 'status',\n",
              " 'law',\n",
              " 'points',\n",
              " 'company',\n",
              " 'otherwise',\n",
              " 'uploaded',\n",
              " 'terms',\n",
              " 'explanation',\n",
              " 'generally',\n",
              " 'sort',\n",
              " 'entire',\n",
              " 'shows',\n",
              " 'description',\n",
              " 'whats',\n",
              " 'recently',\n",
              " 'follow',\n",
              " 'guys',\n",
              " '2008',\n",
              " 'likely',\n",
              " 'film',\n",
              " 'present',\n",
              " 'aware',\n",
              " 'saw',\n",
              " 'definition',\n",
              " 'cited',\n",
              " 'alone',\n",
              " 'google',\n",
              " 'music',\n",
              " 'soon',\n",
              " 'indeed',\n",
              " 'decide',\n",
              " 'ban',\n",
              " 'wp',\n",
              " 'appear',\n",
              " 'views',\n",
              " 'week',\n",
              " 'open',\n",
              " 'citation',\n",
              " 'contributing',\n",
              " 'actual',\n",
              " 'set',\n",
              " 'interesting',\n",
              " 'piece',\n",
              " 'c',\n",
              " 'short',\n",
              " 'white',\n",
              " 'told',\n",
              " 'theory',\n",
              " 'area',\n",
              " 'improve',\n",
              " 'external',\n",
              " 'small',\n",
              " 'story',\n",
              " 'contact',\n",
              " 'simple',\n",
              " '2004',\n",
              " 'various',\n",
              " 'allowed',\n",
              " 'moved',\n",
              " 'test',\n",
              " 'internet',\n",
              " 'obvious',\n",
              " 'family',\n",
              " 'band',\n",
              " 'attention',\n",
              " 'arent',\n",
              " 'proposed',\n",
              " 'jew',\n",
              " 'themselves',\n",
              " 'members',\n",
              " 'wouldnt',\n",
              " 'result',\n",
              " 'disagree',\n",
              " 'thus',\n",
              " 'cunt',\n",
              " 'went',\n",
              " 'type',\n",
              " 'sites',\n",
              " 'ie',\n",
              " 'context',\n",
              " 'mr',\n",
              " 'previous',\n",
              " 'nonsense',\n",
              " 'actions',\n",
              " 'tags',\n",
              " 'cite',\n",
              " 'works',\n",
              " '10',\n",
              " 'citations',\n",
              " 'jews',\n",
              " 'university',\n",
              " 're',\n",
              " 'enjoy',\n",
              " 'conflict',\n",
              " 'hours',\n",
              " 'shouldnt',\n",
              " 'proper',\n",
              " 'bias',\n",
              " 'category',\n",
              " 'job',\n",
              " 'longer',\n",
              " 'file',\n",
              " 'together',\n",
              " 'hell',\n",
              " 'sourced',\n",
              " 'sucks',\n",
              " 'addition',\n",
              " 'happened',\n",
              " 'avoid',\n",
              " 'automatically',\n",
              " 'author',\n",
              " 'valid',\n",
              " 'black',\n",
              " 'creating',\n",
              " 'deal',\n",
              " 'worked',\n",
              " 'npov',\n",
              " 'goes',\n",
              " 'himself',\n",
              " 'seriously',\n",
              " 'john',\n",
              " 'death',\n",
              " 'proof',\n",
              " 'respect',\n",
              " 'bitch',\n",
              " 'science',\n",
              " 'human',\n",
              " 'biased',\n",
              " 'comes',\n",
              " 'helpful',\n",
              " 'large',\n",
              " 'accepted',\n",
              " 'available',\n",
              " 'exist',\n",
              " 'series',\n",
              " 'tildes',\n",
              " 'opinions',\n",
              " 'hand',\n",
              " '6',\n",
              " 'indicate',\n",
              " 'sections',\n",
              " 'rights',\n",
              " 'necessary',\n",
              " 'act',\n",
              " 'meaning',\n",
              " 'attempt',\n",
              " 'accept',\n",
              " 'personally',\n",
              " 'statements',\n",
              " 'violation',\n",
              " 'months',\n",
              " 'criticism',\n",
              " 'accurate',\n",
              " 'action',\n",
              " 'usually',\n",
              " 'unblock',\n",
              " 'german',\n",
              " 'pig',\n",
              " 'cause',\n",
              " 'yeah',\n",
              " 'living',\n",
              " 'copy',\n",
              " 'debate',\n",
              " 'upon',\n",
              " 'assume',\n",
              " 'july',\n",
              " 'calling',\n",
              " 'standard',\n",
              " 'video',\n",
              " 'play',\n",
              " 'rest',\n",
              " 'tagged',\n",
              " 'doubt',\n",
              " 'sex',\n",
              " 'multiple',\n",
              " 'theyre',\n",
              " 'historical',\n",
              " 'serious',\n",
              " 'details',\n",
              " 'dick',\n",
              " 'youll',\n",
              " 'separate',\n",
              " 'manual',\n",
              " 'record',\n",
              " 'blocking',\n",
              " 'afd',\n",
              " 'explaining',\n",
              " 'situation',\n",
              " 'refer',\n",
              " 'wikiproject',\n",
              " 'heard',\n",
              " 'online',\n",
              " 'level',\n",
              " 'fix',\n",
              " 'asking',\n",
              " '7',\n",
              " 'complete',\n",
              " 'speak',\n",
              " 'lack',\n",
              " 'messages',\n",
              " 'none',\n",
              " 'prove',\n",
              " 'third',\n",
              " 'subjects',\n",
              " 'church',\n",
              " 'apparently',\n",
              " '2009',\n",
              " 'south',\n",
              " 'rationale',\n",
              " 'bullshit',\n",
              " 'data',\n",
              " 'directly',\n",
              " 'august',\n",
              " 'period',\n",
              " 'legal',\n",
              " 'behavior',\n",
              " 'difference',\n",
              " 'contribute',\n",
              " 'greek',\n",
              " 'huge',\n",
              " 'gets',\n",
              " 'wikipedian',\n",
              " 'couple',\n",
              " 'supposed',\n",
              " 'among',\n",
              " 'early',\n",
              " 'except',\n",
              " 'march',\n",
              " 'close',\n",
              " 'quality',\n",
              " 'space',\n",
              " 'meant',\n",
              " 'countries',\n",
              " 'run',\n",
              " 'team',\n",
              " 'uses',\n",
              " 'military',\n",
              " 'b',\n",
              " 'changing',\n",
              " 'existing',\n",
              " 'specifically',\n",
              " 'significant',\n",
              " '2010',\n",
              " 'pillars',\n",
              " 'fish',\n",
              " 'incorrect',\n",
              " 'culture',\n",
              " 'described',\n",
              " 'produce',\n",
              " 'jewish',\n",
              " '24',\n",
              " 'uk',\n",
              " 'disruptive',\n",
              " 'd',\n",
              " 'field',\n",
              " 'error',\n",
              " 'india',\n",
              " 'head',\n",
              " 'primary',\n",
              " 'friend',\n",
              " 'earlier',\n",
              " 'sometimes',\n",
              " 'outside',\n",
              " '20',\n",
              " 'purpose',\n",
              " 'administrators',\n",
              " 'modern',\n",
              " 'photo',\n",
              " 'table',\n",
              " 'particularly',\n",
              " 't',\n",
              " 'release',\n",
              " 'gave',\n",
              " 'box',\n",
              " 'cases',\n",
              " 'inclusion',\n",
              " 'born',\n",
              " 'pictures',\n",
              " 'readers',\n",
              " 'june',\n",
              " 'character',\n",
              " 'vote',\n",
              " 'okay',\n",
              " 'groups',\n",
              " 'anonymous',\n",
              " 'abuse',\n",
              " 'arguments',\n",
              " 'business',\n",
              " 'shall',\n",
              " 'sock',\n",
              " 'tutorial',\n",
              " 'january',\n",
              " 'friends',\n",
              " 'numbers',\n",
              " 'control',\n",
              " 'thinking',\n",
              " 'member',\n",
              " 'linked',\n",
              " 'happen',\n",
              " 'reported',\n",
              " 'contest',\n",
              " 'coming',\n",
              " 'takes',\n",
              " 'concerns',\n",
              " 'allow',\n",
              " 'wait',\n",
              " 'majority',\n",
              " 'giving',\n",
              " '8',\n",
              " 'bring',\n",
              " 'eg',\n",
              " 'worth',\n",
              " 'kill',\n",
              " 'totally',\n",
              " 'red',\n",
              " 'force',\n",
              " 'decided',\n",
              " 'discussed',\n",
              " 'house',\n",
              " 'finally',\n",
              " 'absolutely',\n",
              " 'putting',\n",
              " 'scientific',\n",
              " 'respond',\n",
              " 'mistake',\n",
              " 'decision',\n",
              " 'de',\n",
              " 'lost',\n",
              " 'entirely',\n",
              " '100',\n",
              " 'towards',\n",
              " 'merely',\n",
              " 'home',\n",
              " 'neither',\n",
              " 'dear',\n",
              " 'independent',\n",
              " 'international',\n",
              " 'song',\n",
              " 'balls',\n",
              " 'wants',\n",
              " 'possibly',\n",
              " 'unsigned',\n",
              " 'million',\n",
              " 'irrelevant',\n",
              " 'standards',\n",
              " 'april',\n",
              " '12',\n",
              " 'press',\n",
              " 'figure',\n",
              " 'organization',\n",
              " 'looked',\n",
              " 'inappropriate',\n",
              " 'chance',\n",
              " 'posting',\n",
              " 'population',\n",
              " 'advice',\n",
              " 'posts',\n",
              " 'north',\n",
              " 'events',\n",
              " 'unfortunately',\n",
              " 'named',\n",
              " 'album',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of text tokenizing\n",
        "vectorizer(\"Helloo everyone whassup my homies\")[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrX4Qr1y-81",
        "outputId": "5e1bfc0c-3fda-48da-eec5-3464ef815935"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([    1,   523,     1,    29, 37427])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the dataset\n",
        "vectorizedText = vectorizer(x.values)"
      ],
      "metadata": {
        "id": "E08RKn6EznLd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizedText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEtV-tM20HQd",
        "outputId": "243e5516-6c1d-4033-8a77-09c710b5bbb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
              "array([[  645,    76,     2, ...,     0,     0,     0],\n",
              "       [    1,    54,  2489, ...,     0,     0,     0],\n",
              "       [  425,   441,    70, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [32445,  7392,   383, ...,     0,     0,     0],\n",
              "       [    5,    12,   534, ...,     0,     0,     0],\n",
              "       [    5,     8,   130, ...,     0,     0,     0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.data.Dataset.list_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "-mDcQyB5ZAB9",
        "outputId": "8fbfb193-7f7b-4939-fefa-80b3742b840e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.data.ops.dataset_ops.DatasetV2.list_files(file_pattern, shuffle=None, seed=None, name=None) -> 'DatasetV2'>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.dataset_ops.DatasetV2.list_files</b><br/>def list_files(file_pattern, shuffle=None, seed=None, name=None) -&gt; &#x27;DatasetV2&#x27;</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py</a>A dataset of all files matching one or more glob patterns.\n",
              "\n",
              "The `file_pattern` argument should be a small number of glob patterns.\n",
              "If your filenames have already been globbed, use\n",
              "`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
              "filename with `list_files` may result in poor performance with remote\n",
              "storage systems.\n",
              "\n",
              "Note: The default behavior of this method is to return filenames in\n",
              "a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
              "to get results in a deterministic order.\n",
              "\n",
              "Example:\n",
              "  If we had the following files on our filesystem:\n",
              "\n",
              "    - /path/to/dir/a.txt\n",
              "    - /path/to/dir/b.py\n",
              "    - /path/to/dir/c.py\n",
              "\n",
              "  If we pass &quot;/path/to/dir/*.py&quot; as the directory, the dataset\n",
              "  would produce:\n",
              "\n",
              "    - /path/to/dir/b.py\n",
              "    - /path/to/dir/c.py\n",
              "\n",
              "Args:\n",
              "  file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
              "    (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
              "    pattern(s) that will be matched.\n",
              "  shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
              "    Defaults to `True`.\n",
              "  seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
              "    seed that will be used to create the distribution. See\n",
              "    `tf.random.set_seed` for behavior.\n",
              "  name: Optional. A name for the tf.data operations used by `list_files`.\n",
              "\n",
              "Returns:\n",
              " Dataset: A `Dataset` of strings corresponding to file names.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1271);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MCSHBAP - map , cache ,shuffle ,batch ,prefetch from tenserflow slices\n",
        "# batch the data\n",
        "dataset = tf.data.Dataset.from_tensor_slices((vectorizedText,y))# passing the input data and the corresponding labels\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(160000)\n",
        "dataset = dataset.batch(16)\n",
        "dataset = dataset.prefetch(8)# helps prevent bottlenecks\n",
        "#Prefetching enables the system to load the next batch of data while the current batch is being processed by the GPU.\n"
      ],
      "metadata": {
        "id": "VQh8cLUOWqiw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.as_numpy_iterator().next()\n",
        "# load a batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r10p_d_boi2",
        "outputId": "2765c081-2466-4b4c-ffc9-83476757e680"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 2815,   218,   168, ...,     0,     0,     0],\n",
              "        [13904,   774,    11, ...,     0,     0,     0],\n",
              "        [ 2923, 51659,     2, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  637,    51,    96, ...,     0,     0,     0],\n",
              "        [   63,   430,     8, ...,     0,     0,     0],\n",
              "        [    2,   115,   396, ...,     0,     0,     0]]),\n",
              " array([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x , batch_y = dataset.as_numpy_iterator().next()"
      ],
      "metadata": {
        "id": "MdkVotFscgle"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x.shape # inputs\n",
        "batch_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrv_EaLScx6N",
        "outputId": "70ec9a7d-6b62-4e3d-d1c1-a54697ca8d13"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset.take(int(len(dataset )*0.7))\n",
        "val = dataset.skip(int(len(dataset)*0.7)).take(int(len(dataset)*0.2))\n",
        "test = dataset.skip(int(len(dataset)*0.9)).take(int(len(dataset)*0.1))"
      ],
      "metadata": {
        "id": "fh0RsceXdLiG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "GLS8XvWFfcn1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD3r-bpff2fq",
        "outputId": "0d0ceb10-732d-4a4c-876b-44f7c0b35292"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[  561,  8263,  5672, ...,     0,     0,     0],\n",
              "        [   11, 14172,     4, ...,     0,     0,     0],\n",
              "        [  490,     8,   221, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [    8,    39,   151, ...,     0,     0,     0],\n",
              "        [ 5705,     8,   597, ...,     0,     0,     0],\n",
              "        [  288,     2,   121, ...,     0,     0,     0]]),\n",
              " array([[1, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 0, 1, 0],\n",
              "        [1, 0, 1, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create sequencial model"
      ],
      "metadata": {
        "id": "0ZzJZUPwgg75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM , Dropout, Bidirectional,Dense ,Embedding"
      ],
      "metadata": {
        "id": "Sp1tdA27geH7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = Sequential()# instantiate the sequential API so after that we can add the layers to the model?\n",
        "# create the embedding layer\n",
        "\n",
        "model.add(Embedding(MAX_WORDS+1,32)) # +1 is for the unknown words\n",
        "# bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(64 , activation = \"tanh\")))\n",
        "# dense layers\n",
        "model.add(Dense(128,activation = \"relu\"))\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dense(138, activation = \"relu\"))\n",
        "model.add(Dropout(0.3))  # Drop 30% of neurons randomly. Dropout Layer (Before Final Output)\n",
        "\n",
        "# converting the previous dense layer output to values between 0 and 1\n",
        "model.add(Dense(6 , activation = \"sigmoid\"))\n",
        "# dense layers connect each input to each output within its layer"
      ],
      "metadata": {
        "id": "1MTk19oyiDeP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "7w7e7M21wJPr",
        "outputId": "58efaeb0-dc61-44bf-fe78-3989f3584a55"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " embedding (\u001b[38;5;33mEmbedding\u001b[0m)                 ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)         ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)                         ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)                       ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)                       ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)                     ?                                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)                       ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                         </span><span style=\"font-weight: bold\"> Output Shape                </span><span style=\"font-weight: bold\">         Param # </span>\n",
              "\n",
              " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                 ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)         ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     ?                                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model with F1 score , accuracy, recall ,precision Metrics"
      ],
      "metadata": {
        "id": "1CCoMXNU7EAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"f1_score\", **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * (precision * recall) / (precision + recall + epsilon())\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()"
      ],
      "metadata": {
        "id": "y-SdigO42zqb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.backend import epsilon\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\n",
        "        \"accuracy\",\n",
        "        Precision(name=\"precision\"),\n",
        "        Recall(name=\"recall\"),\n",
        "        AUC(name=\"auc\"),\n",
        "        F1Score()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "EusdqYpgrfeQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLOPs integrated training and validation"
      ],
      "metadata": {
        "id": "tR-Y4U7K7XG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLflow Experiment Setup\n",
        "mlflow.set_experiment(\"toxicity_detection\")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Train Model with Logging\n",
        "    history = model.fit(\n",
        "        train,\n",
        "        epochs=20,\n",
        "        validation_data=val,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor=\"val_loss\", patience=3),\n",
        "            WandbMetricsLogger(),  # Logs to W&B\n",
        "            WandbModelCheckpoint(filepath=\"wandb_model.keras\", save_best_only=True)  #  Saves best model\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Retrieve Final Metrics\n",
        "    final_accuracy = history.history[\"accuracy\"][-1]\n",
        "    final_loss = history.history[\"loss\"][-1]\n",
        "\n",
        "    # Log Metrics to MLflow\n",
        "    mlflow.log_param(\"epochs\", 20)\n",
        "    mlflow.log_metric(\"accuracy\", final_accuracy)\n",
        "    mlflow.log_metric(\"loss\", final_loss)\n",
        "\n",
        "    # Save and Log Model\n",
        "    model.save(\"toxicity_model.h5\")\n",
        "    mlflow.log_artifact(\"toxicity_model.h5\")\n",
        "\n",
        "    # Log Epoch-wise Metrics\n",
        "    for epoch in range(len(history.history[\"loss\"])):\n",
        "        mlflow.log_metric(\"train_loss\", history.history[\"loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", history.history[\"val_loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"train_f1\", history.history[\"f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_f1\", history.history[\"val_f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"train_accuracy\", history.history[\"accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", history.history[\"val_accuracy\"][epoch], step=epoch)\n",
        "\n",
        "    # Log Metrics to W&B\n",
        "    wandb.log({\n",
        "        \"accuracy\": final_accuracy,\n",
        "        \"loss\": final_loss\n",
        "    })\n",
        "\n",
        "    # Plot Loss & F1-Score Curves\n",
        "    def plot_and_log_metric(metric_name, title):\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(history.history[metric_name], label=\"Train \" + metric_name.capitalize())\n",
        "        plt.plot(history.history[\"val_\" + metric_name], label=\"Validation \" + metric_name.capitalize())\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(metric_name.capitalize())\n",
        "        plt.legend()\n",
        "        plt.title(title)\n",
        "        plt.savefig(f\"{metric_name}_plot.png\")\n",
        "        mlflow.log_artifact(f\"{metric_name}_plot.png\")  # Log to MLflow\n",
        "        wandb.log({f\"{metric_name}_plot\": wandb.Image(f\"{metric_name}_plot.png\")})  # Log to W&B\n",
        "        plt.close()\n",
        "\n",
        "    # Log Loss & F1-score Plots\n",
        "    plot_and_log_metric(\"loss\", \"Loss over Epochs\")\n",
        "    plot_and_log_metric(\"f1_score\", \"F1 Score over Epochs\")\n",
        "\n",
        "    print(f\"MLflow & W&B logging complete with accuracy: {final_accuracy:.2f}\")\n",
        "\n",
        "# Finish W&B Run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MQmibRQBRyU",
        "outputId": "b0b28de8-6b69-43dc-a5b0-281837420ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9910 - auc: 0.9849 - f1_score: 0.7567 - loss: 0.0447 - precision: 0.8301 - recall: 0.6953 - val_accuracy: 0.9937 - val_auc: 0.9913 - val_f1_score: 0.7823 - val_loss: 0.0396 - val_precision: 0.8483 - val_recall: 0.7259\n",
            "Epoch 2/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9916 - auc: 0.9882 - f1_score: 0.7786 - loss: 0.0388 - precision: 0.8391 - recall: 0.7263 - val_accuracy: 0.9892 - val_auc: 0.9908 - val_f1_score: 0.8000 - val_loss: 0.0350 - val_precision: 0.8684 - val_recall: 0.7415\n",
            "Epoch 3/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9933 - auc: 0.9912 - f1_score: 0.8024 - loss: 0.0349 - precision: 0.8509 - recall: 0.7592 - val_accuracy: 0.9946 - val_auc: 0.9935 - val_f1_score: 0.8105 - val_loss: 0.0310 - val_precision: 0.9038 - val_recall: 0.7347\n",
            "Epoch 4/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 105ms/step - accuracy: 0.9909 - auc: 0.9922 - f1_score: 0.8205 - loss: 0.0326 - precision: 0.8578 - recall: 0.7863 - val_accuracy: 0.9941 - val_auc: 0.9944 - val_f1_score: 0.8369 - val_loss: 0.0303 - val_precision: 0.8451 - val_recall: 0.8289\n",
            "Epoch 5/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9857 - auc: 0.9936 - f1_score: 0.8408 - loss: 0.0296 - precision: 0.8663 - recall: 0.8169 - val_accuracy: 0.9939 - val_auc: 0.9954 - val_f1_score: 0.8530 - val_loss: 0.0267 - val_precision: 0.8964 - val_recall: 0.8135\n",
            "Epoch 6/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9910 - auc: 0.9942 - f1_score: 0.8553 - loss: 0.0278 - precision: 0.8739 - recall: 0.8375 - val_accuracy: 0.9945 - val_auc: 0.9963 - val_f1_score: 0.8720 - val_loss: 0.0246 - val_precision: 0.8756 - val_recall: 0.8684\n",
            "Epoch 7/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9835 - auc: 0.9951 - f1_score: 0.8724 - loss: 0.0250 - precision: 0.8877 - recall: 0.8578 - val_accuracy: 0.9949 - val_auc: 0.9964 - val_f1_score: 0.8899 - val_loss: 0.0227 - val_precision: 0.8898 - val_recall: 0.8900\n",
            "Epoch 8/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 106ms/step - accuracy: 0.9751 - auc: 0.9953 - f1_score: 0.8806 - loss: 0.0232 - precision: 0.8930 - recall: 0.8686 - val_accuracy: 0.9932 - val_auc: 0.9976 - val_f1_score: 0.8997 - val_loss: 0.0210 - val_precision: 0.8873 - val_recall: 0.9124\n",
            "Epoch 9/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 106ms/step - accuracy: 0.9271 - auc: 0.9957 - f1_score: 0.8936 - loss: 0.0213 - precision: 0.9043 - recall: 0.8832 - val_accuracy: 0.8615 - val_auc: 0.9969 - val_f1_score: 0.8911 - val_loss: 0.0200 - val_precision: 0.9128 - val_recall: 0.8704\n",
            "Epoch 10/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.8237 - auc: 0.9964 - f1_score: 0.8989 - loss: 0.0202 - precision: 0.9079 - recall: 0.8902 - val_accuracy: 0.9869 - val_auc: 0.9966 - val_f1_score: 0.9070 - val_loss: 0.0184 - val_precision: 0.9264 - val_recall: 0.8884\n",
            "Epoch 11/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9050 - auc: 0.9963 - f1_score: 0.9083 - loss: 0.0188 - precision: 0.9123 - recall: 0.9043 - val_accuracy: 0.9938 - val_auc: 0.9972 - val_f1_score: 0.9184 - val_loss: 0.0165 - val_precision: 0.9448 - val_recall: 0.8935\n",
            "Epoch 12/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9769 - auc: 0.9969 - f1_score: 0.9157 - loss: 0.0180 - precision: 0.9196 - recall: 0.9120 - val_accuracy: 0.9909 - val_auc: 0.9982 - val_f1_score: 0.9260 - val_loss: 0.0151 - val_precision: 0.9358 - val_recall: 0.9163\n",
            "Epoch 13/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9638 - auc: 0.9976 - f1_score: 0.9252 - loss: 0.0157 - precision: 0.9294 - recall: 0.9211 - val_accuracy: 0.9880 - val_auc: 0.9986 - val_f1_score: 0.9272 - val_loss: 0.0144 - val_precision: 0.9141 - val_recall: 0.9407\n",
            "Epoch 14/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.9739 - auc: 0.9975 - f1_score: 0.9259 - loss: 0.0146 - precision: 0.9263 - recall: 0.9254 - val_accuracy: 0.9897 - val_auc: 0.9984 - val_f1_score: 0.9346 - val_loss: 0.0129 - val_precision: 0.9376 - val_recall: 0.9315\n",
            "Epoch 15/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9573 - auc: 0.9973 - f1_score: 0.9334 - loss: 0.0138 - precision: 0.9342 - recall: 0.9326 - val_accuracy: 0.9907 - val_auc: 0.9985 - val_f1_score: 0.9400 - val_loss: 0.0116 - val_precision: 0.9463 - val_recall: 0.9338\n",
            "Epoch 16/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 105ms/step - accuracy: 0.8470 - auc: 0.9976 - f1_score: 0.9323 - loss: 0.0135 - precision: 0.9332 - recall: 0.9314 - val_accuracy: 0.9783 - val_auc: 0.9985 - val_f1_score: 0.9390 - val_loss: 0.0127 - val_precision: 0.9276 - val_recall: 0.9506\n",
            "Epoch 17/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.8815 - auc: 0.9982 - f1_score: 0.9381 - loss: 0.0124 - precision: 0.9391 - recall: 0.9371 - val_accuracy: 0.8999 - val_auc: 0.9986 - val_f1_score: 0.9405 - val_loss: 0.0108 - val_precision: 0.9363 - val_recall: 0.9448\n",
            "Epoch 18/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.8958 - auc: 0.9978 - f1_score: 0.9394 - loss: 0.0120 - precision: 0.9405 - recall: 0.9383 - val_accuracy: 0.9911 - val_auc: 0.9985 - val_f1_score: 0.9467 - val_loss: 0.0107 - val_precision: 0.9455 - val_recall: 0.9479\n",
            "Epoch 19/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9631 - auc: 0.9983 - f1_score: 0.9454 - loss: 0.0108 - precision: 0.9460 - recall: 0.9449 - val_accuracy: 0.9864 - val_auc: 0.9983 - val_f1_score: 0.9498 - val_loss: 0.0098 - val_precision: 0.9451 - val_recall: 0.9545\n",
            "Epoch 20/20\n",
            "\u001b[1m6981/6981\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 105ms/step - accuracy: 0.9117 - auc: 0.9985 - f1_score: 0.9471 - loss: 0.0104 - precision: 0.9449 - recall: 0.9492 - val_accuracy: 0.9875 - val_auc: 0.9987 - val_f1_score: 0.9531 - val_loss: 0.0090 - val_precision: 0.9552 - val_recall: 0.9510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow & W&B logging complete with accuracy: 0.93\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>epoch/accuracy</td><td></td></tr><tr><td>epoch/auc</td><td></td></tr><tr><td>epoch/epoch</td><td></td></tr><tr><td>epoch/f1_score</td><td></td></tr><tr><td>epoch/learning_rate</td><td></td></tr><tr><td>epoch/loss</td><td></td></tr><tr><td>epoch/precision</td><td></td></tr><tr><td>epoch/recall</td><td></td></tr><tr><td>epoch/val_accuracy</td><td></td></tr><tr><td>epoch/val_auc</td><td></td></tr><tr><td>epoch/val_f1_score</td><td></td></tr><tr><td>epoch/val_loss</td><td></td></tr><tr><td>epoch/val_precision</td><td></td></tr><tr><td>epoch/val_recall</td><td></td></tr><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.92711</td></tr><tr><td>epoch/accuracy</td><td>0.92711</td></tr><tr><td>epoch/auc</td><td>0.99832</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/f1_score</td><td>0.94539</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.01075</td></tr><tr><td>epoch/precision</td><td>0.9446</td></tr><tr><td>epoch/recall</td><td>0.94618</td></tr><tr><td>epoch/val_accuracy</td><td>0.98753</td></tr><tr><td>epoch/val_auc</td><td>0.99868</td></tr><tr><td>epoch/val_f1_score</td><td>0.95305</td></tr><tr><td>epoch/val_loss</td><td>0.00898</td></tr><tr><td>epoch/val_precision</td><td>0.95516</td></tr><tr><td>epoch/val_recall</td><td>0.95095</td></tr><tr><td>loss</td><td>0.01075</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-firebrand-1</strong> at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/i2zqmk3c' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/i2zqmk3c</a><br> View project at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a><br>Synced 5 W&B file(s), 2 media file(s), 40 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250225_015324-i2zqmk3c/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "\n",
        "# Initialize W&B Run (Load previous run)\n",
        "api = wandb.Api()\n",
        "run = api.run(\"toxicity-detection/i2zqmk3c\")\n",
        "history = run.history()\n",
        "print(history.columns)  # Shows all available metric names\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  #\n",
        "\n",
        "# Convert W&B history to match your format\n",
        "history_dict = {\n",
        "    \"loss\": history[\"epoch/loss\"].tolist(),\n",
        "    \"val_loss\": history[\"epoch/val_loss\"].tolist(),\n",
        "    \"f1_score\": history[\"epoch/f1_score\"].tolist(),\n",
        "    \"val_f1_score\": history[\"epoch/val_f1_score\"].tolist(),\n",
        "    \"accuracy\": history[\"epoch/accuracy\"].tolist(),\n",
        "    \"val_accuracy\": history[\"epoch/val_accuracy\"].tolist(),\n",
        "    \"precision\": history[\"epoch/precision\"].tolist(),\n",
        "    \"val_precision\": history[\"epoch/val_precision\"].tolist(),\n",
        "    \"recall\": history[\"epoch/recall\"].tolist(),\n",
        "    \"val_recall\": history[\"epoch/val_recall\"].tolist(),\n",
        "    \"auc\": history[\"epoch/auc\"].tolist(),\n",
        "    \"val_auc\": history[\"epoch/val_auc\"].tolist(),\n",
        "    \"learning_rate\": history[\"epoch/learning_rate\"].tolist(),  # No val version\n",
        "}\n",
        "\n",
        "# Define function to re-plot and log (handling missing val_ cases)\n",
        "def plot_and_log_metric(metric_name, title, has_val=True):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(history_dict[metric_name], label=\"Train \" + metric_name.capitalize())\n",
        "\n",
        "    # Only plot validation if it exists\n",
        "    if has_val:\n",
        "        plt.plot(history_dict[\"val_\" + metric_name], label=\"Validation \" + metric_name.capitalize())\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric_name.capitalize())\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.savefig(f\"{metric_name}_plot.png\")\n",
        "    mlflow.log_artifact(f\"{metric_name}_plot.png\")  # Log to MLflow\n",
        "    wandb.log({f\"{metric_name}_plot\": wandb.Image(f\"{metric_name}_plot.png\")})  # Log to W&B\n",
        "    plt.close()\n",
        "\n",
        "# Recreate and log all plots\n",
        "plot_and_log_metric(\"loss\", \"Loss over Epochs\")\n",
        "plot_and_log_metric(\"f1_score\", \"F1 Score over Epochs\")\n",
        "plot_and_log_metric(\"accuracy\", \"Accuracy over Epochs\")\n",
        "plot_and_log_metric(\"precision\", \"Precision over Epochs\")\n",
        "plot_and_log_metric(\"recall\", \"Recall over Epochs\")\n",
        "plot_and_log_metric(\"auc\", \"AUC over Epochs\")\n",
        "plot_and_log_metric(\"learning_rate\", \"Learning Rate over Epochs\", has_val=False)  # No validation version\n",
        "\n",
        "print(\"Plots regenerated and logged successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F84IdY8A1x2",
        "outputId": "81b48532-6f31-4181-9fcb-f6a6c780bfef"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['loss', '_runtime', 'epoch/val_precision', 'epoch/accuracy',\n",
            "       'epoch/f1_score', 'epoch/val_recall', 'accuracy', 'epoch/auc',\n",
            "       'epoch/learning_rate', '_step', 'epoch/precision', 'epoch/loss',\n",
            "       'epoch/recall', 'epoch/val_auc', 'loss_plot', 'epoch/val_f1_score',\n",
            "       '_timestamp', 'epoch/val_accuracy', 'epoch/epoch', 'f1_score_plot',\n",
            "       'epoch/val_loss'],\n",
            "      dtype='object')\n",
            "Plots regenerated and logged successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from tensorflow.keras.models import load_model  # Correct way to load a Keras model\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model(\"/content/toxicity_model.h5\")\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Or your actual MLflow server URL\n",
        "\n",
        "# End any active MLflow run if it exists\n",
        "mlflow.end_run()\n",
        "\n",
        "# If experiment does not exist, create one\n",
        "experiment_name = \"toxicity_detection\"\n",
        "try:\n",
        "    experiment = mlflow.create_experiment(experiment_name)\n",
        "except:\n",
        "    # If experiment already exists, just get it\n",
        "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "\n",
        "# Start a new run within the experiment\n",
        "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
        "    # Log the model\n",
        "    mlflow.keras.log_model(model, \"model\")\n",
        "\n",
        "    # Assuming you have history_dict available (from your training history)\n",
        "    # Log metrics to MLflow (no retraining required)\n",
        "    for epoch in range(len(history_dict[\"loss\"])):\n",
        "        mlflow.log_metric(\"loss\", history_dict[\"loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", history_dict[\"val_loss\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"accuracy\", history_dict[\"accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", history_dict[\"val_accuracy\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"f1_score\", history_dict[\"f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_f1_score\", history_dict[\"val_f1_score\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"precision\", history_dict[\"precision\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_precision\", history_dict[\"val_precision\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"recall\", history_dict[\"recall\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_recall\", history_dict[\"val_recall\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"auc\", history_dict[\"auc\"][epoch], step=epoch)\n",
        "        mlflow.log_metric(\"val_auc\", history_dict[\"val_auc\"][epoch], step=epoch)\n",
        "\n",
        "    # Log the plots that were saved during training\n",
        "    for metric_name in [\"loss\", \"f1_score\", \"accuracy\", \"precision\", \"recall\", \"auc\"]:\n",
        "        mlflow.log_artifact(f\"{metric_name}_plot.png\")\n",
        "\n",
        "print(\"Model and metrics have been logged successfully.\")\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "\n",
        "# Get experiment by name (or ID)\n",
        "experiment = mlflow.get_experiment_by_name(\"toxicity_detection\")\n",
        "\n",
        "# Retrieve all runs\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "print(runs.columns)\n",
        "\n",
        "# Print the metrics for each run (e.g., accuracy, loss)\n",
        "print(runs[[\"run_id\", \"metrics.accuracy\", \"metrics.loss\"]])\n"
      ],
      "metadata": {
        "id": "knU8GVkrP-uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54987cdd-fdd8-416b-886c-cb78314151af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025/03/06 08:20:45 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " View run gaudy-cub-796 at: http://127.0.0.1:5000/#/experiments/0/runs/3748e07fa1374f3cada78f70bd771cfa\n",
            " View experiment at: http://127.0.0.1:5000/#/experiments/0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/03/06 08:20:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " View run fortunate-loon-507 at: http://127.0.0.1:5000/#/experiments/841964652538824668/runs/9f04ad5c2ba943ba8d94d3630db19bb0\n",
            " View experiment at: http://127.0.0.1:5000/#/experiments/841964652538824668\n",
            "Model and metrics have been logged successfully.\n",
            "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
            "       'end_time', 'metrics.val_accuracy', 'metrics.val_recall',\n",
            "       'metrics.val_auc', 'metrics.recall', 'metrics.loss',\n",
            "       'metrics.precision', 'metrics.val_precision', 'metrics.val_loss',\n",
            "       'metrics.accuracy', 'metrics.val_f1_score', 'metrics.auc',\n",
            "       'metrics.f1_score', 'tags.mlflow.log-model.history',\n",
            "       'tags.mlflow.source.type', 'tags.mlflow.user', 'tags.mlflow.runName',\n",
            "       'tags.mlflow.source.name'],\n",
            "      dtype='object')\n",
            "                             run_id  metrics.accuracy  metrics.loss\n",
            "0  9f04ad5c2ba943ba8d94d3630db19bb0               NaN           NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.keras\n",
        "\n",
        "# Get experiment by name (or ID)\n",
        "experiment = mlflow.get_experiment_by_name(\"toxicity_detection\")\n",
        "\n",
        "# Retrieve all runs\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "print(runs.columns)\n",
        "\n",
        "# Print the metrics for each run (e.g., accuracy, loss)\n",
        "print(runs[[\"run_id\", \"metrics.accuracy\", \"metrics.loss\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgWxs-UKNLoe",
        "outputId": "14ba24a3-9722-4990-8a89-65083c8e9475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
            "       'end_time', 'metrics.loss', 'metrics.auc', 'metrics.val_auc',\n",
            "       'metrics.val_recall', 'metrics.accuracy', 'metrics.f1_score',\n",
            "       'metrics.val_accuracy', 'metrics.val_loss', 'metrics.precision',\n",
            "       'metrics.val_f1_score', 'metrics.val_precision', 'metrics.recall',\n",
            "       'tags.mlflow.source.name', 'tags.mlflow.source.type',\n",
            "       'tags.mlflow.runName', 'tags.mlflow.log-model.history',\n",
            "       'tags.mlflow.user'],\n",
            "      dtype='object')\n",
            "                             run_id  metrics.accuracy  metrics.loss\n",
            "0  a49a4770dcc14ec8b5f24786add8d9f0               NaN           NaN\n",
            "1  86ac75d6a3f44eff836c0aee3efe2804               NaN           NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!mlflow ui --port 5000 &\n",
        "public_url = ngrok.connect(port=\"5000\")\n",
        "print(f\"MLflow Tracking UI: {public_url}\")"
      ],
      "metadata": {
        "id": "N8P2ege9QBlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "\n",
        "custom_objects = {\"LSTM\": LSTM, \"Bidirectional\": Bidirectional}\n",
        "\n",
        "model = tf.keras.models.load_model('/content/toxicity_model.h5', custom_objects=custom_objects)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERYj6qunyPn7",
        "outputId": "ad188185-c6b7-4866-d636-5a6231e6a75d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions"
      ],
      "metadata": {
        "id": "fn5XHZnnClgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = vectorizer(\"Heyy I hate you\")\n",
        "res = model.predict(np.expand_dims(input_str, axis = 0))\n",
        "# printout the columns\n",
        "print(df.columns[2:])\n",
        "print(res)\n",
        "\n",
        "input_str = vectorizer(\"You looks very nice\")\n",
        "res = model.predict(np.expand_dims(input_str, axis = 0))\n",
        "# printout the columns\n",
        "print(df.columns[2:])\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEnDqNuwxYE-",
        "outputId": "eff6c158-2711-414b-dfab-449e38a4cafd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step\n",
            "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
            "       'identity_hate'],\n",
            "      dtype='object')\n",
            "[[0.6889598  0.00483085 0.07420572 0.0524078  0.14637259 0.10724357]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
            "       'identity_hate'],\n",
            "      dtype='object')\n",
            "[[5.3074697e-11 0.0000000e+00 2.5806218e-19 8.3492614e-38 8.7714351e-24\n",
            "  1.4263595e-30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model on Test dataset"
      ],
      "metadata": {
        "id": "xq-HdMxQHnhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X, batch_y = test.as_numpy_iterator().next()\n",
        "(model.predict(batch_X) > 0.5).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahinTbSDmUGC",
        "outputId": "6b75ca85-9100-4a8a-b0e8-2b6197e2b3c8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roLBHIAHmaPT",
        "outputId": "7949dfd1-d452-41a0-afae-fc934915f149"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model on Test Data\n",
        "test_results = model.evaluate(test, verbose=1)\n",
        "\n",
        "# Extract Metrics\n",
        "test_loss = test_results[0]\n",
        "test_accuracy = test_results[1]\n",
        "test_precision = test_results[2]\n",
        "test_recall = test_results[3]\n",
        "test_auc = test_results[4]\n",
        "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall + tf.keras.backend.epsilon())\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Test recall: {test_recall:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "3WUAUnbsHmTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9602b103-3f62-45b6-e68d-f8fed3949756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m997/997\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 178ms/step - accuracy: 0.9877 - auc: 0.9975 - f1_score: 0.2112 - loss: 0.0091 - precision: 0.9523 - recall: 0.9477\n",
            "Test Accuracy: 0.9866\n",
            "Test F1-Score: 0.9498\n",
            "Test recall: 0.9498\n",
            "Test Precision: 0.9498\n",
            "Test loss: 0.0095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure W&B is initialized before logging\n",
        "wandb.init(project=\"toxicity-detection\", name=\"test_evaluation\")\n",
        "\n",
        "# Log Test Metrics to W&B\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy,\n",
        "    \"test_precision\": test_precision,\n",
        "    \"test_recall\": test_recall,\n",
        "    \"test_f1_score\": test_f1,\n",
        "    \"test_auc\": test_auc\n",
        "})\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "dCIjMWsxJtNj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "2f2c4258-e740-43ae-b509-cc30d61f0ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td></td></tr><tr><td>test_auc</td><td></td></tr><tr><td>test_f1_score</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>test_precision</td><td></td></tr><tr><td>test_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.98665</td></tr><tr><td>test_auc</td><td>0.99822</td></tr><tr><td>test_f1_score</td><td>0.94977</td></tr><tr><td>test_loss</td><td>0.00947</td></tr><tr><td>test_precision</td><td>0.94977</td></tr><tr><td>test_recall</td><td>0.94977</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">different-brook-3</strong> at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/vr3nj828' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection/runs/vr3nj828</a><br> View project at: <a href='https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection' target=\"_blank\">https://wandb.ai/e20189-university-of-peradeniya/toxicity-detection</a><br>Synced 5 W&B file(s), 27 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250306_002806-vr3nj828/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrs_MdtcJtU0",
        "outputId": "6fe663d1-3671-4132-d79f-268fbcdad9aa"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "app =FastAPI()\n",
        "\n",
        "model =tf.keras.models.load_model(\"/content/toxicity_model.h5\")\n",
        "class InputText(BaseModel):\n",
        "  text:str\n",
        "@app.post(\"/predict\")\n",
        "async def predict(input_text : InputText):\n",
        "  text: input_text.text\n",
        "  vect_text = vectorizer(text)\n",
        "  prediction = model.predict([text])\n",
        "  return {\"prediction\": prediction.tolist()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRSCH5bcJ7zp",
        "outputId": "d303c910-9fd3-412a-f49f-166bf5f90ca6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install dependencies if not installed\n",
        "!pip install fastapi uvicorn pyngrok mlflow tensorflow\n",
        "\n",
        "# Step 2: Run FastAPI in the background\n",
        "!nohup uvicorn server:app --host 0.0.0.0 --port 8000 --reload &\n",
        "\n",
        "# Step 3: Start ngrok for FastAPI\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2tVdIx30fQWA14bNtDn4lfBR9tE_77bGYGjjdXZRXD17uKHy5\")  # Replace with your token\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI is running at {public_url}\")\n",
        "\n",
        "# Step 4: Start MLflow UI\n",
        "!nohup mlflow ui --port 5000 &\n",
        "\n",
        "# Step 5: Expose MLflow UI with ngrok\n",
        "mlflow_url = ngrok.connect(5000)\n",
        "print(f\"MLflow UI is available at {mlflow_url}\")\n"
      ],
      "metadata": {
        "id": "xo8LYgHHPydc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95a53f0-c36c-4707-ca9b-84b2077757d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.20.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: mlflow-skinny==2.20.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.20.3)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.44.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.37b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.6.1)\n",
            "nohup: appending output to 'nohup.out'\n",
            "FastAPI is running at NgrokTunnel: \"https://9b99-34-59-186-82.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "nohup: appending output to 'nohup.out'\n",
            "MLflow UI is available at NgrokTunnel: \"https://297c-34-59-186-82.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "\n",
        "# Initialize W&B Run (Load previous run)\n",
        "api = wandb.Api()\n",
        "run = api.run(\"toxicity-detection/i2zqmk3c\")\n",
        "history = run.history()\n",
        "print(history.columns)  # Shows all available metric names\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # Or your actual MLflow server URL\n",
        "\n",
        "\n",
        "# Convert W&B history to match your format\n",
        "history_dict = {\n",
        "    \"loss\": history[\"epoch/loss\"].tolist(),\n",
        "    \"val_loss\": history[\"epoch/val_loss\"].tolist(),\n",
        "    \"f1_score\": history[\"epoch/f1_score\"].tolist(),\n",
        "    \"val_f1_score\": history[\"epoch/val_f1_score\"].tolist(),\n",
        "    \"accuracy\": history[\"epoch/accuracy\"].tolist(),\n",
        "    \"val_accuracy\": history[\"epoch/val_accuracy\"].tolist(),\n",
        "    \"precision\": history[\"epoch/precision\"].tolist(),\n",
        "    \"val_precision\": history[\"epoch/val_precision\"].tolist(),\n",
        "    \"recall\": history[\"epoch/recall\"].tolist(),\n",
        "    \"val_recall\": history[\"epoch/val_recall\"].tolist(),\n",
        "    \"auc\": history[\"epoch/auc\"].tolist(),\n",
        "    \"val_auc\": history[\"epoch/val_auc\"].tolist(),\n",
        "    \"learning_rate\": history[\"epoch/learning_rate\"].tolist(),  # No val version\n",
        "}\n",
        "\n",
        "# Define function to re-plot and log (handling missing val_ cases)\n",
        "def plot_and_log_metric(metric_name, title, has_val=True):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(history_dict[metric_name], label=\"Train \" + metric_name.capitalize())\n",
        "\n",
        "    # Only plot validation if it exists\n",
        "    if has_val:\n",
        "        plt.plot(history_dict[\"val_\" + metric_name], label=\"Validation \" + metric_name.capitalize())\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric_name.capitalize())\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.savefig(f\"{metric_name}_plot.png\")\n",
        "    mlflow.log_artifact(f\"{metric_name}_plot.png\")  # Log to MLflow\n",
        "    wandb.log({f\"{metric_name}_plot\": wandb.Image(f\"{metric_name}_plot.png\")})  # Log to W&B\n",
        "    plt.close()\n",
        "\n",
        "# Recreate and log all plots\n",
        "plot_and_log_metric(\"loss\", \"Loss over Epochs\")\n",
        "plot_and_log_metric(\"f1_score\", \"F1 Score over Epochs\")\n",
        "plot_and_log_metric(\"accuracy\", \"Accuracy over Epochs\")\n",
        "plot_and_log_metric(\"precision\", \"Precision over Epochs\")\n",
        "plot_and_log_metric(\"recall\", \"Recall over Epochs\")\n",
        "plot_and_log_metric(\"auc\", \"AUC over Epochs\")\n",
        "plot_and_log_metric(\"learning_rate\", \"Learning Rate over Epochs\", has_val=False)  # No validation version\n",
        "\n",
        "print(\"Plots regenerated and logged successfully.\")\n"
      ],
      "metadata": {
        "id": "6ltwmxfNhfm8",
        "outputId": "579e70e5-8c06-452c-c19e-d0b9625a77e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['loss', '_runtime', 'epoch/val_precision', 'epoch/accuracy',\n",
            "       'epoch/f1_score', 'epoch/val_recall', 'accuracy', 'epoch/auc',\n",
            "       'epoch/learning_rate', '_step', 'epoch/precision', 'epoch/loss',\n",
            "       'epoch/recall', 'epoch/val_auc', 'loss_plot', 'epoch/val_f1_score',\n",
            "       '_timestamp', 'epoch/val_accuracy', 'epoch/epoch', 'f1_score_plot',\n",
            "       'epoch/val_loss'],\n",
            "      dtype='object')\n",
            "Plots regenerated and logged successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken 2tVdIx30fQWA14bNtDn4lfBR9tE_77bGYGjjdXZRXD17uKHy5\n",
        "\n",
        "!uvicorn --host 0.0.0.0 --port 8000 --reload app:app\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"FastAPI app is accessible at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-PeZpZpM06m",
        "outputId": "f0f37e1c-9753-41c6-c7e7-e668060cdb33"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-03-06T07:52:08+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m22521\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"app\".\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m22521\u001b[0m]\n",
            "FastAPI app is accessible at: NgrokTunnel: \"https://a3b5-34-59-186-82.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run():\n",
        "    # Log Test Metrics\n",
        "    mlflow.log_metric(\"test_loss\", test_loss)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_precision\", test_precision)\n",
        "    mlflow.log_metric(\"test_recall\", test_recall)\n",
        "    mlflow.log_metric(\"test_auc\", test_auc)\n",
        "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
        "\n",
        "    print(\" Test results logged in MLflow\")\n"
      ],
      "metadata": {
        "id": "d3oOVU-eJ2fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fc9afd-49ae-43c5-edcc-e98334fe4f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test results logged in MLflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test using Gradio UI and Mflow integration"
      ],
      "metadata": {
        "id": "rvxLDEbolsUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok mlflow tensorflow gradio websockets\n"
      ],
      "metadata": {
        "id": "v8BlN-SqkdZC",
        "outputId": "4a05dd59-365c-4e29-8fb2-ae72130321d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.20.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (14.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: mlflow-skinny==2.20.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.20.3)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.44.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.4.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.37b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.6.1)\n",
            "Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 tomlkit-0.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "7fb1b39587a140ed81871042baa6a879"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, WebSocket\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "import mlflow\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model(\"toxicity_model.h5\")\n",
        "\n",
        "# Load vectorizer\n",
        "MAX_WORDS = 200000\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_WORDS, output_sequence_length=1800, output_mode='int')\n",
        "\n",
        "if os.path.exists(\"vectorizer_vocab.pkl\"):\n",
        "    with open(\"vectorizer_vocab.pkl\", \"rb\") as f:\n",
        "        vocab = pickle.load(f)\n",
        "    vectorizer.set_vocabulary(vocab)\n",
        "\n",
        "# Start FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# MLflow Experiment Setup\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_experiment(\"Toxicity Detection\")\n",
        "\n",
        "# Function to Predict Toxicity & Log to MLflow\n",
        "def predict_toxicity(message):\n",
        "    with mlflow.start_run():\n",
        "        input_text = tf.expand_dims(message, axis=0)\n",
        "        input_text = vectorizer(input_text)\n",
        "        prediction = model.predict(input_text)[0]\n",
        "\n",
        "        # Log inputs & predictions to MLflow\n",
        "        mlflow.log_param(\"input_text\", message)\n",
        "        for label, score in zip([\"Toxic\", \"Severe Toxic\", \"Obscene\", \"Threat\", \"Insult\", \"Identity Hate\"], prediction):\n",
        "            mlflow.log_metric(label, float(score))\n",
        "\n",
        "        return {label: round(float(score), 2) for label, score in zip([\"Toxic\", \"Severe Toxic\", \"Obscene\", \"Threat\", \"Insult\", \"Identity Hate\"], prediction)}\n",
        "\n",
        "@app.websocket(\"/chat\")\n",
        "async def chatroom(websocket: WebSocket):\n",
        "    await websocket.accept()\n",
        "    while True:\n",
        "        data = await websocket.receive_text()\n",
        "        message_data = json.loads(data)\n",
        "        message = message_data[\"message\"]\n",
        "\n",
        "        # Predict toxicity & log to MLflow\n",
        "        toxicity_scores = predict_toxicity(message)\n",
        "\n",
        "        await websocket.send_json({\"message\": message, \"toxicity_scores\": toxicity_scores})\n"
      ],
      "metadata": {
        "id": "MpsCwP3oLxNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082226dc-3f61-410c-ec84-5ed92f70e684"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025/03/06 09:15:56 INFO mlflow.tracking.fluent: Experiment with name 'Toxicity Detection' does not exist. Creating a new experiment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup uvicorn server:app --host 0.0.0.0 --port 8000 --reload &\n"
      ],
      "metadata": {
        "id": "0cBmqJm5a6_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e5b7e8-8c4f-4ed1-c210-0db2cf71087c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI is running at {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8plwfwX5XJu-",
        "outputId": "9e3bfe1a-d5ed-476b-f51c-ab64fffd9525"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI is running at NgrokTunnel: \"https://7ef4-34-59-186-82.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -a ngrok\n",
        "!killall ngrok\n"
      ],
      "metadata": {
        "id": "EJqeP_Z2wIPd",
        "outputId": "2685482d-9a8f-411b-b30a-e6b04a97caae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26140 /root/.config/ngrok/ngrok start --none --log=stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup mlflow ui --port 5000 &\n",
        "mlflow_url = ngrok.connect(5000)\n",
        "print(f\"MLflow UI is available at {mlflow_url}\")\n"
      ],
      "metadata": {
        "id": "VyEOV0wovzv6",
        "outputId": "debee6c0-3ee4-4a5e-9e5f-dc1819c4b9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "MLflow UI is available at NgrokTunnel: \"https://0d8e-34-59-186-82.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import websockets\n",
        "import asyncio\n",
        "import json\n",
        "\n",
        "WEBSOCKET_URL = \"https://0d8e-34-59-186-82.ngrok-free.app/\"  # Replace with your FastAPI ngrok URL\n",
        "\n",
        "async def send_message(message, history, user):\n",
        "    if not user.strip():\n",
        "        return history + [{\"role\": \"assistant\", \"content\": \" Please enter a username before sending messages.\"}]\n",
        "\n",
        "    async with websockets.connect(WEBSOCKET_URL) as websocket:\n",
        "        user_message = {\"user\": user, \"message\": message}\n",
        "        await websocket.send(json.dumps(user_message))\n",
        "        response = await websocket.recv()\n",
        "        response_data = json.loads(response)\n",
        "\n",
        "        history.append({\"role\": \"user\", \"content\": f\" {user}: {message}\"})\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\" Assistant: {response_data['message']} (Toxicity: {response_data['toxicity_scores']})\"})\n",
        "        return history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    username_input = gr.Textbox(label=\"Enter Your Name\", placeholder=\"Type your name here...\")\n",
        "    chat_interface = gr.Chatbot([], type=\"messages\", label=\"Toxicity Detection Chatroom\")\n",
        "    msg_input = gr.Textbox(label=\"Type your message here:\")\n",
        "    send_btn = gr.Button(\"Send\")\n",
        "\n",
        "    send_btn.click(fn=send_message, inputs=[msg_input, chat_interface, username_input], outputs=chat_interface)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "lEOm84m-XmNJ",
        "outputId": "ffe376e4-b038-444d-d49e-a8592907bab0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cb97fca029e4b842a4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cb97fca029e4b842a4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}